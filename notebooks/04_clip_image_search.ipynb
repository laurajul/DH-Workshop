{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "id": "intro-md",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity Search with CLIP\n\nIn this notebook, you will:\n\n1. **Load** the CLIP embeddings computed in Notebook 03\n2. **Select or upload** a query image\n3. **Find** museum artworks that are visually similar to your image\n4. **Explore** how CLIP understands visual similarity\n\n---\n\n## Text Search vs. Image Search\n\n| | Text Search (Notebook 03) | Image Search (this notebook) |\n|--|--------------------------|------------------------------|\n| **Query** | `\"a river landscape\"` | üñºÔ∏è any image file |\n| **Encoded by** | CLIP text encoder | CLIP image encoder |\n| **Finds** | images matching the description | images with similar visual content |\n| **Use case** | explore by concept | \"find more like this\" |\n\nBoth use the same embedding space ‚Äî the query vector just comes from a different encoder.\n"
   ]
  },
  {
   "id": "setup-md",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 1: Setup"
   ]
  },
  {
   "id": "imports",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image as PILImage\nfrom IPython.display import display\nimport ipywidgets as widgets\n\ntry:\n    import torch\n    import clip\n    CLIP_AVAILABLE = True\n    print(\"‚úì CLIP loaded!\")\nexcept ImportError:\n    CLIP_AVAILABLE = False\n    print(\"‚ùå CLIP not installed ‚Äî pip install git+https://github.com/openai/CLIP.git torch torchvision\")\n\nif CLIP_AVAILABLE:\n    if torch.cuda.is_available():\n        DEVICE = 'cuda'\n        print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        DEVICE = 'mps'\n        print(\"‚úì Apple Silicon GPU (MPS)\")\n    else:\n        DEVICE = 'cpu'\n        print(\"‚ÑπÔ∏è  CPU mode\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "paths",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "CURRENT_DIR  = Path.cwd()\nPROJECT_ROOT = CURRENT_DIR.parent\nDATA_DIR     = PROJECT_ROOT / \"data\"\n\n# ============================================================\n# CONFIGURATION ‚Äî must match Notebooks 02 & 03!\n# ============================================================\n\nCOLLECTION_NAME = \"Museum of Gothenburg\"\nTHEME           = \"art\"\nSEARCH_KEYWORD  = None\nMODEL_NAME      = \"ViT-B/32\"\n\n# ============================================================\n\n_parts = [COLLECTION_NAME.replace(' ', '_')]\nif THEME:          _parts.append(THEME)\nif SEARCH_KEYWORD: _parts.append(SEARCH_KEYWORD)\n_folder = \"_\".join(_parts)\n\nCOLLECTION_IMAGES_DIR      = DATA_DIR / \"images\"     / _folder\nCOLLECTION_EMBEDDINGS_FILE = DATA_DIR / \"embeddings\" / _folder / f\"{_folder}_clip_embeddings.npz\"\nCLIP_MODEL_DIR             = PROJECT_ROOT / \"models\" / \"CLIP\"\n\nprint(f\"Images     : {COLLECTION_IMAGES_DIR}\")\nprint(f\"Embeddings : {COLLECTION_EMBEDDINGS_FILE}\")\nprint(f\"Exists     : {COLLECTION_EMBEDDINGS_FILE.exists()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "load-md",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 2: Load Model and Embeddings"
   ]
  },
  {
   "id": "load-clip",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if CLIP_AVAILABLE:\n    print(f\"Loading '{MODEL_NAME}' from {CLIP_MODEL_DIR}...\")\n    model, preprocess = clip.load(MODEL_NAME, device=DEVICE,\n                                  download_root=str(CLIP_MODEL_DIR))\n    model.eval()\n    print(f\"‚úì Model ready on {DEVICE}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "select-collection",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Scan data/embeddings/ for collections that have computed embeddings\n_emb_root = DATA_DIR / \"embeddings\"\n_available = sorted([\n    d.name for d in _emb_root.iterdir()\n    if d.is_dir() and any(d.glob(\"*.npz\"))\n]) if _emb_root.exists() else []\n\nif not _available:\n    print(\"‚ö†Ô∏è  No embeddings found in data/embeddings/\")\n    print(\"   Run Notebook 03 first to compute embeddings.\")\n    collection_selector = None\nelse:\n    _default = _folder if _folder in _available else _available[0]\n\n    collection_selector = widgets.Dropdown(\n        options=_available,\n        value=_default,\n        description=\"Collection:\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"550px\"),\n    )\n\n    _info = widgets.HTML(value=\"\")\n\n    def _update_info(change=None):\n        sel     = collection_selector.value\n        npz     = next((_emb_root / sel).glob(\"*.npz\"), None)\n        img_dir = DATA_DIR / \"images\" / sel\n        n_emb   = \"\"\n        if npz:\n            try:\n                d = __import__(\"numpy\").load(npz, allow_pickle=True)\n                n_emb = f\"{len(d['filenames'])} embeddings\"\n            except Exception:\n                n_emb = \"embeddings file unreadable\"\n        n_img = len(list(img_dir.glob(\"*.jpg\"))) if img_dir.exists() else 0\n        _info.value = (\n            f\"<span style='color:grey'>{n_emb}\"\n            + (f\" ¬∑ {n_img} images locally\" if n_img else \" ¬∑ images not downloaded\")\n            + \"</span>\"\n        )\n\n    collection_selector.observe(lambda c: _update_info(c), names=\"value\")\n    _update_info()\n\n    display(widgets.VBox([collection_selector, _info]))\n    print(\"Select a collection, then run the next cell to load its embeddings.\")\n"
   ]
  },
  {
   "id": "load-embeddings",
   "cell_type": "code",
   "metadata": {},
   "source": "# Resolve paths from the dropdown above (falls back to paths cell if widget not shown)\n_sel = collection_selector.value if collection_selector else _folder\n\nCOLLECTION_IMAGES_DIR      = DATA_DIR / \"images\"     / _sel\nCOLLECTION_EMBEDDINGS_FILE = next((DATA_DIR / \"embeddings\" / _sel).glob(\"*.npz\"), None)\n\nprint(f\"Collection : {_sel}\")\nprint(f\"Images     : {COLLECTION_IMAGES_DIR}\")\nprint(f\"Embeddings : {COLLECTION_EMBEDDINGS_FILE}\")\nprint()\n\nif COLLECTION_EMBEDDINGS_FILE and COLLECTION_EMBEDDINGS_FILE.exists():\n    data = np.load(COLLECTION_EMBEDDINGS_FILE, allow_pickle=True)\n    image_embeddings = torch.tensor(data['embeddings'], dtype=torch.float32).to(DEVICE)\n    image_filenames  = data['filenames']\n    print(f\"‚úì Loaded {len(image_filenames)} embeddings  {image_embeddings.shape}\")\nelse:\n    image_embeddings = None\n    image_filenames  = None\n    print(\"‚ö†Ô∏è  No embeddings found ‚Äî run Notebook 03 first.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "query-md",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 3: Choose a Query Image\n\nYou can use **any image** as your query ‚Äî a painting, a photograph, even a smartphone photo.\nCLIP encodes it into the same 512-dim space as the collection images and finds the closest matches.\n\n### Option A ‚Äî Upload a file (works in Colab & JupyterLab)\n"
   ]
  },
  {
   "id": "upload-image",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Option A: upload from your computer\nupload = widgets.FileUpload(accept='image/*', multiple=False)\ndisplay(upload)\nprint(\"Click 'Upload' and select an image file, then run the next cell.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "load-uploaded",
   "cell_type": "code",
   "metadata": {},
   "source": "import io\n\n# Run this after uploading a file above\nif upload.value:\n    file_info = upload.value[0]          # ipywidgets 8+: tuple of dicts\n    name      = file_info['name']\n    img_bytes = file_info['content']\n    query_image = PILImage.open(io.BytesIO(img_bytes)).convert('RGB')\n    print(f\"‚úì Loaded: {name}  ({query_image.size[0]}√ó{query_image.size[1]})\")\n    display(query_image.resize((300, int(300 * query_image.size[1] / query_image.size[0]))))\nelse:\n    query_image = None\n    print(\"No file uploaded yet.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "option-b-md",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B ‚Äî Use an image already in the collection\n\nPick any filename from the collection as the query. Useful for \"find more like this\".\n"
   ]
  },
  {
   "id": "pick-from-collection",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# Option B: pick a file from the collection by index or name\n# ============================================================\n\nQUERY_INDEX = 0   # <-- change this (0 = first image in the collection)\n# Or set a specific filename:\n# QUERY_FILENAME = \"some_file.jpg\"\n\n# ============================================================\n\nif image_filenames is not None:\n    query_name  = str(image_filenames[QUERY_INDEX])\n    query_path  = COLLECTION_IMAGES_DIR / Path(query_name).name\n    if query_path.exists():\n        query_image = PILImage.open(query_path).convert('RGB')\n        print(f\"‚úì Query image: {query_name}\")\n        display(query_image.resize((300, int(300 * query_image.size[1] / query_image.size[0]))))\n    else:\n        print(f\"‚ö†Ô∏è  File not found: {query_path}\")\n        query_image = None\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "search-md",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 4: Find Similar Images"
   ]
  },
  {
   "id": "image-search",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def image_similarity_search(query_img, top_k=10, exclude_self=True):\n    \"\"\"\n    Find images in the collection most similar to query_img.\n\n    Parameters:\n        query_img:    PIL Image to use as the query\n        top_k:        Number of results\n        exclude_self: If True, skip the exact query image (useful for Option B)\n\n    Returns:\n        List of (filename, similarity_score) tuples\n    \"\"\"\n    if image_embeddings is None:\n        print(\"‚ùå No embeddings ‚Äî run the load cell above\")\n        return []\n\n    # Encode the query image with CLIP\n    with torch.no_grad():\n        tensor    = preprocess(query_img).unsqueeze(0).to(DEVICE)\n        query_emb = model.encode_image(tensor).float()\n        query_emb = query_emb / query_emb.norm(dim=-1, keepdim=True)\n\n    # Dot product against all collection embeddings\n    sims = (image_embeddings @ query_emb.T).squeeze()\n\n    # Sort and take top-k\n    top_idx = sims.argsort(descending=True)\n    results = []\n    for idx in top_idx:\n        fname = str(image_filenames[idx.item()])\n        score = sims[idx].item()\n        if exclude_self and score > 0.9999:\n            continue          # skip near-identical match (the image itself)\n        results.append((fname, score))\n        if len(results) >= top_k:\n            break\n\n    return results\n\n\nif CLIP_AVAILABLE and image_embeddings is not None and query_image is not None:\n    print(\"üîç Searching for similar images...\")\n    similar = image_similarity_search(query_image, top_k=10)\n\n    print(f\"\\nTop {len(similar)} most similar images:\\n\")\n    for i, (fname, score) in enumerate(similar, 1):\n        print(f\"{i:2}. {score:.4f}  {Path(fname).name[:60]}\")\nelse:\n    print(\"‚ùå Make sure CLIP is loaded, embeddings are loaded, and a query image is selected.\")\n    similar = []\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "display-similar",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display the most similar images\ndef show_similar(results, max_show=5):\n    for fname, score in results[:max_show]:\n        img_path = COLLECTION_IMAGES_DIR / Path(fname).name\n        print(f\"\\nSimilarity: {score:.4f}  |  {Path(fname).name[:60]}\")\n        if img_path.exists():\n            img = PILImage.open(img_path)\n            ratio = min(350 / img.width, 350 / img.height)\n            if ratio < 1:\n                img = img.resize((int(img.width*ratio), int(img.height*ratio)),\n                                 PILImage.Resampling.LANCZOS)\n            display(img)\n        else:\n            print(\"  [file not found locally]\")\n\nif similar:\n    show_similar(similar, max_show=5)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "explore-md",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 5: Explore\n\n- Try **different query images** ‚Äî what makes two artworks \"similar\" to CLIP?\n- Compare CLIP similarity to your own intuition\n- Use a **modern photo** as the query ‚Äî can CLIP find historical artworks that match?\n"
   ]
  },
  {
   "id": "summary-md",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Summary\n\nIn this notebook, you:\n\n1. **Used an image as a query** ‚Äî encoding it with CLIP's image encoder\n2. **Found similar artworks** by dot-product similarity in the shared embedding space\n3. **Explored** what \"visual similarity\" means to a neural network\n\n### How It Differs from Text Search\n\nBoth text and image search land in the same embedding space.\nThe only difference is *how the query is encoded* ‚Äî text encoder vs. image encoder.\nThis is what makes CLIP so powerful: you can mix-and-match query types.\n\n### Further Ideas\n\n- **Cross-modal**: encode a text description, find similar images, then use the top image\n  as a new query to find even more similar images\n- **Clustering**: group embeddings by cosine similarity to discover visual themes\n  in the collection without any labels\n"
   ]
  }
 ]
}