{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# Semantic Image Search with CLIP\n",
    "\n",
    "In this notebook, you will learn how to use **CLIP** (Contrastive Language-Image Pre-training) to search through art collections using natural language.\n",
    "\n",
    "## What is CLIP?\n",
    "\n",
    "CLIP is a neural network trained by OpenAI that learns to connect images and text. It can:\n",
    "\n",
    "- **Understand images** by converting them into numerical representations (embeddings)\n",
    "- **Understand text** by converting descriptions into the same embedding space\n",
    "- **Match** images and text by measuring how similar their embeddings are\n",
    "\n",
    "This allows us to search for images using natural language descriptions like:\n",
    "- \"a painting of a stormy sea\"\n",
    "- \"portrait of a woman in red\"\n",
    "- \"winter landscape with snow\"\n",
    "- \"flowers in a vase\"\n",
    "\n",
    "## Two Modes\n",
    "\n",
    "This notebook supports two modes:\n",
    "\n",
    "1. **Pre-calculated Mode** (Recommended for most laptops)\n",
    "   - Uses pre-computed image embeddings\n",
    "   - Only needs to compute text embeddings (fast on any device)\n",
    "   - Perfect for workshop settings\n",
    "\n",
    "2. **Full Mode** (For powerful GPUs)\n",
    "   - Computes image embeddings on-the-fly\n",
    "   - Allows processing your own images\n",
    "   - Requires CUDA GPU for reasonable speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup\n",
    "\n",
    "First, let's install and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CLIP if not already installed\n",
    "# Uncomment the line below if you need to install\n",
    "\n",
    "# !pip install git+https://github.com/openai/CLIP.git pillow torch torchvision numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import CLIP\n",
    "try:\n",
    "    import clip\n",
    "    print(f\"CLIP loaded successfully!\")\n",
    "except ImportError:\n",
    "    print(\"CLIP not installed. Please run: pip install git+https://github.com/openai/CLIP.git\")\n",
    "\n",
    "# Check for GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU (this will be slower for image encoding).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"../\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "IMAGES_DIR = PROJECT_ROOT / \"images\"\n",
    "EMBEDDINGS_FILE = DATA_DIR / \"clip_embeddings.npz\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Embeddings file: {EMBEDDINGS_FILE}\")\n",
    "print(f\"Embeddings exist: {EMBEDDINGS_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mode-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Choose Your Mode\n",
    "\n",
    "Select the mode based on your hardware:\n",
    "\n",
    "| Mode | Requirements | Speed | Use when... |\n",
    "|------|--------------|-------|-------------|\n",
    "| `precalculated` | Pre-computed embeddings file | Fast | Workshop setting, any laptop |\n",
    "| `full` | CUDA GPU with 4GB+ VRAM | Slower | You have a powerful GPU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mode-select",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION: Choose your mode\n",
    "# ============================================================\n",
    "\n",
    "# Options: 'precalculated' or 'full'\n",
    "MODE = 'precalculated'  # <-- CHANGE THIS if you have a powerful GPU\n",
    "\n",
    "# CLIP model to use (must match pre-calculated embeddings if using that mode)\n",
    "# Options: 'ViT-B/32' (fastest), 'ViT-B/16' (better), 'ViT-L/14' (best but slowest)\n",
    "MODEL_NAME = 'ViT-B/32'\n",
    "\n",
    "# Device: 'cuda' for GPU, 'cpu' for CPU\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print(f\"Mode: {MODE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load CLIP Model\n",
    "\n",
    "Load the CLIP model. This will download the model weights on first run (~350MB for ViT-B/32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading CLIP model '{MODEL_NAME}' on {DEVICE}...\")\n",
    "model, preprocess = clip.load(MODEL_NAME, device=DEVICE)\n",
    "model.eval()\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Get embedding dimension\n",
    "with torch.no_grad():\n",
    "    dummy_text = clip.tokenize([\"test\"]).to(DEVICE)\n",
    "    dummy_embedding = model.encode_text(dummy_text)\n",
    "    EMBEDDING_DIM = dummy_embedding.shape[1]\n",
    "    print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Load or Compute Image Embeddings\n",
    "\n",
    "Depending on your mode, either load pre-calculated embeddings or compute them from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-precalculated",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store embeddings and filenames\n",
    "image_embeddings = None\n",
    "image_filenames = None\n",
    "images_base_dir = None\n",
    "\n",
    "if MODE == 'precalculated':\n",
    "    # Load pre-calculated embeddings\n",
    "    if not EMBEDDINGS_FILE.exists():\n",
    "        print(f\"ERROR: Pre-calculated embeddings not found at {EMBEDDINGS_FILE}\")\n",
    "        print(\"Please ask the workshop instructor for the embeddings file,\")\n",
    "        print(\"or switch to MODE = 'full' if you have a GPU.\")\n",
    "    else:\n",
    "        print(f\"Loading pre-calculated embeddings from {EMBEDDINGS_FILE}...\")\n",
    "        data = np.load(EMBEDDINGS_FILE, allow_pickle=True)\n",
    "        \n",
    "        image_embeddings = data['embeddings']\n",
    "        image_filenames = data['filenames']\n",
    "        saved_model = str(data.get('model_name', 'unknown'))\n",
    "        \n",
    "        print(f\"Loaded {len(image_filenames)} image embeddings\")\n",
    "        print(f\"Embeddings shape: {image_embeddings.shape}\")\n",
    "        print(f\"Embeddings were computed with model: {saved_model}\")\n",
    "        \n",
    "        if saved_model != MODEL_NAME:\n",
    "            print(f\"WARNING: Current model ({MODEL_NAME}) differs from saved ({saved_model})!\")\n",
    "            print(\"Results may be less accurate. Consider using the same model.\")\n",
    "        \n",
    "        # Convert to torch tensor for faster computation\n",
    "        image_embeddings = torch.tensor(image_embeddings, dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        # Load the index to find the images directory\n",
    "        index_file = EMBEDDINGS_FILE.with_suffix('.json')\n",
    "        if index_file.exists():\n",
    "            with open(index_file) as f:\n",
    "                index_data = json.load(f)\n",
    "            print(f\"Index loaded: {index_data.get('num_images', '?')} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'full':\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    # Set the images directory\n",
    "    # Change this to your downloaded images folder\n",
    "    images_base_dir = IMAGES_DIR / \"all_images\"  # <-- CHANGE THIS to your images folder\n",
    "    \n",
    "    if not images_base_dir.exists():\n",
    "        print(f\"Images directory not found: {images_base_dir}\")\n",
    "        print(\"Please download images first using the 01_api_and_data notebook,\")\n",
    "        print(\"or update the path above.\")\n",
    "    else:\n",
    "        # Find all images\n",
    "        image_files = []\n",
    "        for ext in ['.jpg', '.jpeg', '.png', '.webp']:\n",
    "            image_files.extend(images_base_dir.rglob(f'*{ext}'))\n",
    "            image_files.extend(images_base_dir.rglob(f'*{ext.upper()}'))\n",
    "        image_files = sorted(set(image_files))\n",
    "        \n",
    "        print(f\"Found {len(image_files)} images\")\n",
    "        \n",
    "        if len(image_files) > 0:\n",
    "            # Compute embeddings\n",
    "            print(\"Computing image embeddings (this may take a while)...\")\n",
    "            \n",
    "            all_embeddings = []\n",
    "            all_filenames = []\n",
    "            batch_size = 32 if DEVICE == 'cuda' else 8\n",
    "            \n",
    "            for i in tqdm(range(0, len(image_files), batch_size)):\n",
    "                batch_files = image_files[i:i + batch_size]\n",
    "                batch_images = []\n",
    "                batch_names = []\n",
    "                \n",
    "                for img_path in batch_files:\n",
    "                    try:\n",
    "                        image = Image.open(img_path).convert('RGB')\n",
    "                        image_tensor = preprocess(image)\n",
    "                        batch_images.append(image_tensor)\n",
    "                        batch_names.append(str(img_path.relative_to(images_base_dir)))\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                if batch_images:\n",
    "                    batch_tensor = torch.stack(batch_images).to(DEVICE)\n",
    "                    with torch.no_grad():\n",
    "                        embeddings = model.encode_image(batch_tensor)\n",
    "                        embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
    "                    all_embeddings.append(embeddings)\n",
    "                    all_filenames.extend(batch_names)\n",
    "            \n",
    "            image_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "            image_filenames = np.array(all_filenames)\n",
    "            \n",
    "            print(f\"Computed embeddings for {len(image_filenames)} images\")\n",
    "            print(f\"Embeddings shape: {image_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Text Embedding and Semantic Search\n",
    "\n",
    "Now the fun part! We'll create functions to:\n",
    "1. Convert your search query into an embedding\n",
    "2. Find the most similar images using cosine similarity\n",
    "\n",
    "### Understanding Cosine Similarity\n",
    "\n",
    "Cosine similarity measures how similar two vectors are by calculating the cosine of the angle between them:\n",
    "\n",
    "- **1.0** = Identical direction (very similar)\n",
    "- **0.0** = Perpendicular (unrelated)\n",
    "- **-1.0** = Opposite direction (very different)\n",
    "\n",
    "CLIP embeddings are normalized, so we can compute cosine similarity as a simple dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search-funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text_query: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a text query into a CLIP embedding.\n",
    "    \n",
    "    Args:\n",
    "        text_query: Natural language description\n",
    "    \n",
    "    Returns:\n",
    "        Normalized embedding tensor\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize([text_query]).to(DEVICE)\n",
    "        text_embedding = model.encode_text(text_tokens)\n",
    "        # Normalize\n",
    "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
    "    return text_embedding\n",
    "\n",
    "\n",
    "def encode_text_ensemble(text_queries: list) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encode multiple text queries and average their embeddings.\n",
    "    \n",
    "    This technique (prompt ensembling) often gives better results\n",
    "    than using a single query.\n",
    "    \n",
    "    Args:\n",
    "        text_queries: List of related descriptions\n",
    "    \n",
    "    Returns:\n",
    "        Averaged, normalized embedding tensor\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize(text_queries).to(DEVICE)\n",
    "        text_embeddings = model.encode_text(text_tokens)\n",
    "        # Average the embeddings\n",
    "        mean_embedding = text_embeddings.mean(dim=0, keepdim=True)\n",
    "        # Normalize\n",
    "        mean_embedding = mean_embedding / mean_embedding.norm(dim=-1, keepdim=True)\n",
    "    return mean_embedding\n",
    "\n",
    "\n",
    "def search_images(text_embedding: torch.Tensor, top_k: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Find the most similar images to a text embedding.\n",
    "    \n",
    "    Args:\n",
    "        text_embedding: The query embedding\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (filename, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity (dot product of normalized vectors)\n",
    "    similarities = (image_embeddings @ text_embedding.T).squeeze()\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = similarities.argsort(descending=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        filename = image_filenames[idx.item()] if isinstance(image_filenames[idx.item()], str) else str(image_filenames[idx.item()])\n",
    "        score = similarities[idx].item()\n",
    "        results.append((filename, score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def semantic_search(query: str, top_k: int = 10, use_ensemble: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Perform semantic search using a natural language query.\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language search query\n",
    "        top_k: Number of results to return\n",
    "        use_ensemble: If True, create variations of the query for better results\n",
    "    \n",
    "    Returns:\n",
    "        List of (filename, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    if use_ensemble:\n",
    "        # Create query variations for better matching\n",
    "        queries = [\n",
    "            query,\n",
    "            f\"a painting of {query}\",\n",
    "            f\"an artwork depicting {query}\",\n",
    "            f\"a photo of {query}\",\n",
    "            f\"{query}, fine art\"\n",
    "        ]\n",
    "        text_embedding = encode_text_ensemble(queries)\n",
    "    else:\n",
    "        text_embedding = encode_text(query)\n",
    "    \n",
    "    return search_images(text_embedding, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "display-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Display Search Results\n",
    "\n",
    "Helper functions to display images from search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_path(filename: str) -> Path:\n",
    "    \"\"\"\n",
    "    Find the full path to an image file.\n",
    "    \n",
    "    Searches in common locations based on the mode.\n",
    "    \"\"\"\n",
    "    # Try different base directories\n",
    "    possible_bases = [\n",
    "        images_base_dir,\n",
    "        IMAGES_DIR,\n",
    "        IMAGES_DIR / \"all_images\",\n",
    "        PROJECT_ROOT / \"downloaded_data\"\n",
    "    ]\n",
    "    \n",
    "    for base in possible_bases:\n",
    "        if base is None:\n",
    "            continue\n",
    "        full_path = base / filename\n",
    "        if full_path.exists():\n",
    "            return full_path\n",
    "    \n",
    "    # Try searching recursively\n",
    "    for base in possible_bases:\n",
    "        if base is None or not base.exists():\n",
    "            continue\n",
    "        matches = list(base.rglob(Path(filename).name))\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def display_results(results: list, max_display: int = 5, show_scores: bool = True):\n",
    "    \"\"\"\n",
    "    Display search results as images.\n",
    "    \n",
    "    Args:\n",
    "        results: List of (filename, score) tuples from search\n",
    "        max_display: Maximum number of images to show\n",
    "        show_scores: Whether to display similarity scores\n",
    "    \"\"\"\n",
    "    displayed = 0\n",
    "    \n",
    "    for filename, score in results[:max_display]:\n",
    "        img_path = find_image_path(filename)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        if show_scores:\n",
    "            print(f\"Similarity: {score:.4f}\")\n",
    "        print(f\"File: {filename}\")\n",
    "        \n",
    "        if img_path and img_path.exists():\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                # Resize for display\n",
    "                max_size = 500\n",
    "                ratio = min(max_size / img.width, max_size / img.height)\n",
    "                if ratio < 1:\n",
    "                    new_size = (int(img.width * ratio), int(img.height * ratio))\n",
    "                    img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "                display(img)\n",
    "                displayed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Could not display image: {e}\")\n",
    "        else:\n",
    "            print(f\"Image file not found locally: {filename}\")\n",
    "            print(\"(Image embeddings exist but images may not be downloaded)\")\n",
    "    \n",
    "    if displayed == 0:\n",
    "        print(\"\\nNo images could be displayed.\")\n",
    "        print(\"Make sure you have downloaded the images using the 01_api_and_data notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise: Semantic Image Search\n",
    "\n",
    "**Your task:** Change the `SEARCH_QUERY` variable to search for different subjects!\n",
    "\n",
    "### Tips for Better Queries\n",
    "\n",
    "| Instead of... | Try... | Why |\n",
    "|---------------|--------|-----|\n",
    "| `water` | `shoreline with waves` | More specific, less ambiguous |\n",
    "| `woman` | `portrait of a young woman` | Provides context |\n",
    "| `forest` | `dense pine forest in summer` | Adds detail |\n",
    "| `sad` | `melancholic expression, somber mood` | Describes the feeling |\n",
    "\n",
    "### Suggested Queries to Try\n",
    "\n",
    "**Nature:**\n",
    "- `\"a lake surrounded by mountains\"`\n",
    "- `\"stormy sea with waves\"`\n",
    "- `\"birch trees in autumn\"`\n",
    "- `\"snowy winter landscape\"`\n",
    "\n",
    "**People:**\n",
    "- `\"portrait of an elderly man\"`\n",
    "- `\"children playing outdoors\"`\n",
    "- `\"peasants working in a field\"`\n",
    "- `\"woman reading a book\"`\n",
    "\n",
    "**Subjects:**\n",
    "- `\"flowers in a vase, still life\"`\n",
    "- `\"sailing ships at sea\"`\n",
    "- `\"mythological scene\"`\n",
    "- `\"interior of a church\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE: Change the search query below!\n",
    "# ============================================================\n",
    "\n",
    "SEARCH_QUERY = \"shoreline with waves and beach\"  # <-- CHANGE THIS!\n",
    "\n",
    "# Number of results to return\n",
    "TOP_K = 10\n",
    "\n",
    "# Use prompt ensembling for potentially better results\n",
    "USE_ENSEMBLE = True\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print(f\"Searching for: '{SEARCH_QUERY}'\")\n",
    "print(f\"Using ensemble: {USE_ENSEMBLE}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search\n",
    "if image_embeddings is not None:\n",
    "    results = semantic_search(SEARCH_QUERY, top_k=TOP_K, use_ensemble=USE_ENSEMBLE)\n",
    "    \n",
    "    print(f\"Top {len(results)} results:\")\n",
    "    print()\n",
    "    for i, (filename, score) in enumerate(results, 1):\n",
    "        print(f\"{i:2}. {score:.4f} - {filename}\")\n",
    "else:\n",
    "    print(\"No image embeddings loaded. Please check the setup above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "if image_embeddings is not None and results:\n",
    "    display_results(results, max_display=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Advanced - Custom Prompt Engineering\n",
    "\n",
    "For more control, you can create your own list of prompts. This is useful when:\n",
    "- A single query doesn't capture what you're looking for\n",
    "- You want to combine multiple concepts\n",
    "- You want to exclude certain things (though CLIP doesn't support negation well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ADVANCED: Create custom prompts\n",
    "# ============================================================\n",
    "\n",
    "# Define multiple prompts that describe what you're looking for\n",
    "CUSTOM_PROMPTS = [\n",
    "    \"a painting of water near the shore\",\n",
    "    \"beach scene with ocean waves\",\n",
    "    \"coastal landscape with sea\",\n",
    "    \"shoreline at sunset\",\n",
    "    \"rocky beach with water\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "if image_embeddings is not None:\n",
    "    print(\"Using custom prompts:\")\n",
    "    for p in CUSTOM_PROMPTS:\n",
    "        print(f\"  - {p}\")\n",
    "    print()\n",
    "    \n",
    "    # Encode and average the prompts\n",
    "    custom_embedding = encode_text_ensemble(CUSTOM_PROMPTS)\n",
    "    \n",
    "    # Search\n",
    "    custom_results = search_images(custom_embedding, top_k=10)\n",
    "    \n",
    "    print(f\"Top 10 results:\")\n",
    "    for i, (filename, score) in enumerate(custom_results, 1):\n",
    "        print(f\"{i:2}. {score:.4f} - {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display custom search results\n",
    "if image_embeddings is not None:\n",
    "    display_results(custom_results, max_display=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Compare Different Queries\n",
    "\n",
    "See how different phrasings affect search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different ways to search for the same concept\n",
    "QUERIES_TO_COMPARE = [\n",
    "    \"water\",\n",
    "    \"sea\",\n",
    "    \"ocean waves\",\n",
    "    \"shoreline with beach\"\n",
    "]\n",
    "\n",
    "if image_embeddings is not None:\n",
    "    print(\"Comparing queries - showing top result for each:\\n\")\n",
    "    \n",
    "    for query in QUERIES_TO_COMPARE:\n",
    "        results = semantic_search(query, top_k=1, use_ensemble=False)\n",
    "        if results:\n",
    "            filename, score = results[0]\n",
    "            print(f\"'{query}'\")\n",
    "            print(f\"  -> {score:.4f} - {filename}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Save Search Results\n",
    "\n",
    "Export your search results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_search_results(results: list, query: str, output_dir: Path = None):\n",
    "    \"\"\"\n",
    "    Save search results to a JSON file.\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = DATA_DIR\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create safe filename from query\n",
    "    safe_query = \"\".join(c for c in query if c.isalnum() or c in ' _-')[:50]\n",
    "    filename = f\"search_results_{safe_query.replace(' ', '_')}.json\"\n",
    "    \n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    data = {\n",
    "        'query': query,\n",
    "        'model': MODEL_NAME,\n",
    "        'results': [\n",
    "            {'filename': f, 'similarity': s}\n",
    "            for f, s in results\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(results)} results to {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your search results\n",
    "if image_embeddings is not None and 'results' in dir():\n",
    "    save_search_results(results, SEARCH_QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Batch Search and Filtering\n",
    "\n",
    "Find images matching a query and copy them to a folder (like the original experiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def filter_images_by_query(\n",
    "    query: str,\n",
    "    output_dir: Path,\n",
    "    threshold: float = 0.25,\n",
    "    max_images: int = 100,\n",
    "    use_ensemble: bool = True,\n",
    "    copy_files: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Find and optionally copy all images matching a query above a threshold.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        output_dir: Directory to copy matching images to\n",
    "        threshold: Minimum similarity score (0-1)\n",
    "        max_images: Maximum number of images to return\n",
    "        use_ensemble: Use prompt ensembling\n",
    "        copy_files: Whether to copy files or just list them\n",
    "    \n",
    "    Returns:\n",
    "        List of matching (filename, score) tuples\n",
    "    \"\"\"\n",
    "    # Get all results above threshold\n",
    "    results = semantic_search(query, top_k=len(image_filenames), use_ensemble=use_ensemble)\n",
    "    \n",
    "    # Filter by threshold\n",
    "    matches = [(f, s) for f, s in results if s >= threshold][:max_images]\n",
    "    \n",
    "    print(f\"Found {len(matches)} images with similarity >= {threshold}\")\n",
    "    \n",
    "    if copy_files and matches:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        copied = 0\n",
    "        for filename, score in matches:\n",
    "            src_path = find_image_path(filename)\n",
    "            if src_path and src_path.exists():\n",
    "                dst_path = output_dir / Path(filename).name\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                copied += 1\n",
    "        \n",
    "        print(f\"Copied {copied} images to {output_dir}\")\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE: Filter and collect images\n",
    "# ============================================================\n",
    "\n",
    "FILTER_QUERY = \"shoreline with waves and beach\"  # <-- CHANGE THIS!\n",
    "SIMILARITY_THRESHOLD = 0.25  # Adjust: higher = stricter matching\n",
    "MAX_IMAGES = 50\n",
    "OUTPUT_FOLDER = IMAGES_DIR / \"filtered_shorelines\"  # <-- CHANGE THIS!\n",
    "\n",
    "# Set to True to actually copy files, False to just preview\n",
    "COPY_FILES = False  # <-- Change to True when ready\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "if image_embeddings is not None:\n",
    "    matches = filter_images_by_query(\n",
    "        query=FILTER_QUERY,\n",
    "        output_dir=OUTPUT_FOLDER,\n",
    "        threshold=SIMILARITY_THRESHOLD,\n",
    "        max_images=MAX_IMAGES,\n",
    "        copy_files=COPY_FILES\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop matches:\")\n",
    "    for f, s in matches[:10]:\n",
    "        print(f\"  {s:.4f} - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **What CLIP is** and how it connects images and text\n",
    "2. **How to use pre-calculated embeddings** for fast search on any device\n",
    "3. **How cosine similarity works** for measuring image-text similarity\n",
    "4. **Prompt engineering techniques** for better search results\n",
    "5. **How to filter large image collections** using semantic search\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **More specific queries** generally give better results than single words\n",
    "- **Prompt ensembling** (averaging multiple descriptions) improves robustness\n",
    "- **Threshold tuning** is important - start low and increase until results look good\n",
    "- CLIP understands **visual concepts** beyond what metadata keywords capture\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- CLIP doesn't handle **negation** well (\"not a portrait\" won't work)\n",
    "- Results depend on **training data** (CLIP was trained on internet images)\n",
    "- **Fine-grained distinctions** (e.g., specific art styles) may be challenging\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different CLIP models (ViT-L/14 is more accurate but slower)\n",
    "- Combine CLIP search with metadata filtering\n",
    "- Use CLIP for image clustering or visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
