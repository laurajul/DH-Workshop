{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": "# Advanced: Compute CLIP Embeddings from Your Own Images\n\nThis notebook allows you to compute CLIP embeddings for your own image collection. This is useful when:\n\n- You have a collection of images you want to make searchable\n- You're working with a dataset not covered by pre-calculated embeddings\n- You want to use a different CLIP model\n\n**Hardware:**\n- **NVIDIA GPU (CUDA)** ‚Äî fastest (~1‚Äì5 sec per 100 images)\n- **Apple Silicon GPU (MPS)** ‚Äî good performance on M1/M2/M3 Macs\n- **CPU** ‚Äî works fine for small collections; expect ~1‚Äì5 min per 100 images\n- Disk space: ~4 MB per 1 000 images (ViT-B/32)\n\n---\n\n## How It Works\n\n```mermaid\nflowchart TD\n    subgraph Input\n        FOLDER[\"üìÅ Image Folder\\n(jpg, png, webp...)\"]\n    end\n    \n    subgraph Processing\n        LOAD[\"Load images\\nin batches\"]\n        PREP[\"Preprocess\\n(resize, normalize)\"]\n        ENC[\"üß† CLIP\\nImage Encoder\"]\n    end\n    \n    subgraph Output\n        EMB[\"üíæ embeddings.npz\\n(numpy arrays)\"]\n        IDX[\"üìã index.json\\n(filenames)\"]\n    end\n    \n    FOLDER --> LOAD\n    LOAD --> PREP\n    PREP --> ENC\n    ENC --> EMB\n    ENC --> IDX\n```\n\nThe output files can then be used with Notebooks 02 and 03 for semantic search."
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "# Standard library imports\nimport os\nimport json\nimport time\nfrom pathlib import Path\n\n# External libraries\nimport numpy as np\nfrom PIL import Image as PILImage\nfrom tqdm.notebook import tqdm\n\n# Import PyTorch and CLIP\ntry:\n    import torch\n    import clip\n    CLIP_AVAILABLE = True\n    print(f\"‚úì CLIP loaded successfully!\")\nexcept ImportError:\n    CLIP_AVAILABLE = False\n    print(\"‚ùå CLIP not installed.\")\n    print(\"   Install with: pip install git+https://github.com/openai/CLIP.git torch torchvision\")\n\n# Select compute device: CUDA GPU > Apple Silicon GPU > CPU\nif CLIP_AVAILABLE:\n    if torch.cuda.is_available():\n        DEVICE = 'cuda'\n        gpu_name = torch.cuda.get_device_name(0)\n        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"‚úì NVIDIA GPU (CUDA): {gpu_name}\")\n        print(f\"  Memory: {gpu_mem:.1f} GB\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        DEVICE = 'mps'\n        print(\"‚úì Apple Silicon GPU (MPS) ‚Äî good performance!\")\n    else:\n        DEVICE = 'cpu'\n        print(\"‚ÑπÔ∏è No GPU detected. Using CPU.\")\n        print(\"   Expect ~1‚Äì5 minutes per 100 images. Fine for small collections.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": "# Set up paths\nCURRENT_DIR = Path.cwd()\nPROJECT_ROOT = CURRENT_DIR.parent\n\nprint(f\"Project root: {PROJECT_ROOT}\")"
  },
  {
   "cell_type": "markdown",
   "id": "config-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Configuration\n",
    "\n",
    "### Choose Your CLIP Model\n",
    "\n",
    "| Model | Embedding Dim | Speed | Quality | VRAM |\n",
    "|-------|---------------|-------|---------|------|\n",
    "| `RN50` | 1024 | Fast | Good | ~2 GB |\n",
    "| `RN101` | 512 | Medium | Better | ~3 GB |\n",
    "| `ViT-B/32` | 512 | Fast | Good | ~2 GB |\n",
    "| `ViT-B/16` | 512 | Medium | Better | ~3 GB |\n",
    "| `ViT-L/14` | 768 | Slow | Best | ~5 GB |\n",
    "| `ViT-L/14@336px` | 768 | Slowest | Best+ | ~6 GB |\n",
    "\n",
    "**Recommendation:** Start with `ViT-B/32` for a good balance of speed and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CONFIGURATION - Adjust these settings!\n# ============================================================\n\n# Collection to compute embeddings for\nCOLLECTION_NAME = \"Uppsala University\"  # <-- CHANGE THIS!\n\n# CLIP model to use\nMODEL_NAME = 'ViT-B/32'  # <-- Options: RN50, ViT-B/32, ViT-B/16, ViT-L/14\n\n# Batch size (reduce if you get out-of-memory errors)\nBATCH_SIZE = 32 if DEVICE == 'cuda' else 8\n\n# ============================================================\n\nsafe_name = COLLECTION_NAME.lower().replace(' ', '_')\nIMAGES_FOLDER      = PROJECT_ROOT / \"data\" / \"images\"     / COLLECTION_NAME\nOUTPUT_EMBEDDINGS  = PROJECT_ROOT / \"data\" / \"embeddings\" / f\"{safe_name}_clip_embeddings.npz\"\n\nprint(f\"Collection:    {COLLECTION_NAME}\")\nprint(f\"Images folder: {IMAGES_FOLDER}\")\nprint(f\"Output file:   {OUTPUT_EMBEDDINGS}\")\nprint(f\"Model:         {MODEL_NAME}\")\nprint(f\"Batch size:    {BATCH_SIZE}\")\nprint(f\"Device:        {DEVICE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the images folder\n",
    "if IMAGES_FOLDER.exists():\n",
    "    # Find all images\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp']\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(IMAGES_FOLDER.rglob(f'*{ext}'))\n",
    "        image_files.extend(IMAGES_FOLDER.rglob(f'*{ext.upper()}'))\n",
    "    \n",
    "    image_files = sorted(set(image_files))\n",
    "    \n",
    "    print(f\"‚úì Found {len(image_files)} images in {IMAGES_FOLDER}\")\n",
    "    \n",
    "    if image_files:\n",
    "        print(f\"\\nFirst 5 images:\")\n",
    "        for f in image_files[:5]:\n",
    "            print(f\"  - {f.name}\")\n",
    "        if len(image_files) > 5:\n",
    "            print(f\"  ... and {len(image_files) - 5} more\")\n",
    "else:\n",
    "    print(f\"‚ùå Folder not found: {IMAGES_FOLDER}\")\n",
    "    print(\"   Please update IMAGES_FOLDER to point to your images.\")\n",
    "    image_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compute-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLIP_AVAILABLE:\n",
    "    print(f\"Loading CLIP model '{MODEL_NAME}'...\")\n",
    "    print(\"(This may download the model on first run, ~350MB)\")\n",
    "    \n",
    "    model, preprocess = clip.load(MODEL_NAME, device=DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get embedding dimension\n",
    "    with torch.no_grad():\n",
    "        dummy_text = clip.tokenize([\"test\"]).to(DEVICE)\n",
    "        dummy_embedding = model.encode_text(dummy_text)\n",
    "        EMBEDDING_DIM = dummy_embedding.shape[1]\n",
    "    \n",
    "    print(f\"‚úì Model loaded on {DEVICE}\")\n",
    "    print(f\"  Embedding dimension: {EMBEDDING_DIM}\")\n",
    "else:\n",
    "    print(\"‚ùå CLIP not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Compute Embeddings\n",
    "\n",
    "This is the main computation. Depending on your hardware and number of images, this may take:\n",
    "- **GPU:** ~1-5 seconds per 100 images\n",
    "- **CPU:** ~1-5 minutes per 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(image_files, batch_size=32):\n",
    "    \"\"\"\n",
    "    Compute CLIP embeddings for a list of image files.\n",
    "    \n",
    "    Parameters:\n",
    "        image_files: List of Path objects to images\n",
    "        batch_size: Number of images to process at once\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (embeddings array, filenames list)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    all_filenames = []\n",
    "    errors = []\n",
    "    \n",
    "    num_batches = (len(image_files) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images in {num_batches} batches...\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx in tqdm(range(0, len(image_files), batch_size), desc=\"Processing batches\"):\n",
    "        batch_files = image_files[batch_idx:batch_idx + batch_size]\n",
    "        batch_images = []\n",
    "        batch_names = []\n",
    "        \n",
    "        # Load and preprocess batch\n",
    "        for img_path in batch_files:\n",
    "            try:\n",
    "                image = PILImage.open(img_path).convert('RGB')\n",
    "                image_tensor = preprocess(image)\n",
    "                batch_images.append(image_tensor)\n",
    "                # Store relative path from images folder\n",
    "                try:\n",
    "                    rel_path = img_path.relative_to(IMAGES_FOLDER)\n",
    "                except ValueError:\n",
    "                    rel_path = img_path.name\n",
    "                batch_names.append(str(rel_path))\n",
    "            except Exception as e:\n",
    "                errors.append((str(img_path), str(e)))\n",
    "                continue\n",
    "        \n",
    "        if not batch_images:\n",
    "            continue\n",
    "        \n",
    "        # Compute embeddings\n",
    "        batch_tensor = torch.stack(batch_images).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode_image(batch_tensor)\n",
    "            # Normalize embeddings\n",
    "            embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_filenames.extend(batch_names)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        if DEVICE == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Combine all embeddings\n",
    "    if all_embeddings:\n",
    "        embeddings_array = np.concatenate(all_embeddings, axis=0)\n",
    "    else:\n",
    "        embeddings_array = np.array([])\n",
    "    \n",
    "    print(f\"\\n‚úì Computed {len(all_filenames)} embeddings in {elapsed:.1f} seconds\")\n",
    "    print(f\"  Speed: {len(all_filenames) / elapsed:.1f} images/second\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n‚ö†Ô∏è {len(errors)} images failed to process:\")\n",
    "        for path, err in errors[:5]:\n",
    "            print(f\"  - {Path(path).name}: {err[:50]}\")\n",
    "        if len(errors) > 5:\n",
    "            print(f\"  ... and {len(errors) - 5} more\")\n",
    "    \n",
    "    return embeddings_array, all_filenames, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN THE COMPUTATION\n",
    "# ============================================================\n",
    "\n",
    "# Set to True when ready to compute\n",
    "RUN_COMPUTATION = False  # <-- Change to True when ready!\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "if RUN_COMPUTATION and CLIP_AVAILABLE and image_files:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STARTING EMBEDDING COMPUTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    embeddings, filenames, errors = compute_embeddings(image_files, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(f\"\\nFinal embeddings shape: {embeddings.shape}\")\n",
    "else:\n",
    "    if not RUN_COMPUTATION:\n",
    "        print(\"‚ÑπÔ∏è Computation skipped. Set RUN_COMPUTATION = True to proceed.\")\n",
    "    elif not CLIP_AVAILABLE:\n",
    "        print(\"‚ùå CLIP not available\")\n",
    "    else:\n",
    "        print(\"‚ùå No images found\")\n",
    "    \n",
    "    embeddings = None\n",
    "    filenames = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embeddings is not None and len(embeddings) > 0:\n",
    "    # Create output directory\n",
    "    OUTPUT_EMBEDDINGS.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save embeddings as numpy archive\n",
    "    print(f\"Saving embeddings to {OUTPUT_EMBEDDINGS}...\")\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        OUTPUT_EMBEDDINGS,\n",
    "        embeddings=embeddings,\n",
    "        filenames=np.array(filenames),\n",
    "        model_name=MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    file_size = OUTPUT_EMBEDDINGS.stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úì Saved! File size: {file_size:.2f} MB\")\n",
    "    \n",
    "    # Save index JSON for easy reference\n",
    "    index_file = OUTPUT_EMBEDDINGS.with_suffix('.json')\n",
    "    index_data = {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'embedding_dim': int(embeddings.shape[1]),\n",
    "        'num_images': len(filenames),\n",
    "        'source_folder': str(IMAGES_FOLDER),\n",
    "        'filenames': filenames\n",
    "    }\n",
    "    \n",
    "    with open(index_file, 'w') as f:\n",
    "        json.dump(index_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Index saved to {index_file}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"DONE! Your embeddings are ready to use.\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTo use these embeddings in Notebooks 02 or 03:\")\n",
    "    print(f\"  1. Update EMBEDDINGS_FILE to point to:\")\n",
    "    print(f\"     {OUTPUT_EMBEDDINGS}\")\n",
    "    print(f\"  2. Update IMAGES_DIR to point to:\")\n",
    "    print(f\"     {IMAGES_FOLDER}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No embeddings to save. Run the computation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Verify Embeddings\n",
    "\n",
    "Let's verify the saved embeddings work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved embeddings\n",
    "if OUTPUT_EMBEDDINGS.exists():\n",
    "    print(\"Verifying saved embeddings...\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    data = np.load(OUTPUT_EMBEDDINGS, allow_pickle=True)\n",
    "    \n",
    "    loaded_embeddings = data['embeddings']\n",
    "    loaded_filenames = data['filenames']\n",
    "    loaded_model = str(data.get('model_name', 'unknown'))\n",
    "    \n",
    "    print(f\"‚úì Loaded successfully!\")\n",
    "    print(f\"  Embeddings shape: {loaded_embeddings.shape}\")\n",
    "    print(f\"  Number of images: {len(loaded_filenames)}\")\n",
    "    print(f\"  Model: {loaded_model}\")\n",
    "    \n",
    "    # Quick sanity check - embeddings should be normalized\n",
    "    norms = np.linalg.norm(loaded_embeddings, axis=1)\n",
    "    print(f\"  Embedding norms: min={norms.min():.4f}, max={norms.max():.4f}, mean={norms.mean():.4f}\")\n",
    "    \n",
    "    if np.allclose(norms, 1.0, atol=0.01):\n",
    "        print(\"  ‚úì Embeddings are properly normalized\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Embeddings may not be normalized\")\n",
    "else:\n",
    "    print(f\"‚ùå Embeddings file not found: {OUTPUT_EMBEDDINGS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search with the new embeddings\n",
    "if OUTPUT_EMBEDDINGS.exists() and CLIP_AVAILABLE:\n",
    "    print(\"Testing semantic search...\")\n",
    "    \n",
    "    # Load embeddings as torch tensor\n",
    "    data = np.load(OUTPUT_EMBEDDINGS, allow_pickle=True)\n",
    "    test_embeddings = torch.tensor(data['embeddings'], dtype=torch.float32).to(DEVICE)\n",
    "    test_filenames = data['filenames']\n",
    "    \n",
    "    # Test query\n",
    "    test_query = \"landscape with water\"\n",
    "    \n",
    "    # Encode query\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize([test_query]).to(DEVICE)\n",
    "        text_embedding = model.encode_text(text_tokens)\n",
    "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Search\n",
    "    similarities = (test_embeddings @ text_embedding.T).squeeze()\n",
    "    top_indices = similarities.argsort(descending=True)[:5]\n",
    "    \n",
    "    print(f\"\\nTest query: '{test_query}'\")\n",
    "    print(f\"Top 5 results:\")\n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        filename = str(test_filenames[idx.item()])\n",
    "        score = similarities[idx].item()\n",
    "        print(f\"  {i}. {score:.4f} - {filename[:50]}\")\n",
    "    \n",
    "    print(\"\\n‚úì Semantic search working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Configure** CLIP embedding computation\n",
    "2. **Choose** an appropriate CLIP model\n",
    "3. **Compute** embeddings for an image collection\n",
    "4. **Save** embeddings in a reusable format\n",
    "5. **Verify** the embeddings work correctly\n",
    "\n",
    "### Output Files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `embeddings.npz` | Numpy archive with embeddings and filenames |\n",
    "| `embeddings.json` | Index file with metadata |\n",
    "\n",
    "### Using Your Embeddings\n",
    "\n",
    "To use your custom embeddings in other notebooks:\n",
    "\n",
    "```python\n",
    "# Load your embeddings\n",
    "data = np.load('path/to/your/embeddings.npz', allow_pickle=True)\n",
    "embeddings = data['embeddings']\n",
    "filenames = data['filenames']\n",
    "model_name = str(data.get('model_name', 'unknown'))\n",
    "```\n",
    "\n",
    "### Tips for Large Collections\n",
    "\n",
    "- **Memory management:** Reduce batch size if you run out of GPU memory\n",
    "- **Checkpointing:** For very large collections, save intermediate results\n",
    "- **Model selection:** ViT-B/32 is fastest; ViT-L/14 is best quality\n",
    "- **Storage:** ~4MB per 1000 images (ViT-B/32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}