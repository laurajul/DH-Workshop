{
 "cells": [
  {
   "cell_type": "code",
   "id": "colab-mount-drive",
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "# If already mounted this will show \"Drive is already mounted\" ‚Äî that's fine.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "colab-install-deps",
   "metadata": {},
   "source": [
    "# Install packages that are not pre-installed in Colab\n",
    "# (torch, torchvision, numpy, Pillow, requests are already available)\n",
    "!pip install -q git+https://github.com/openai/CLIP.git ftfy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# Image Similarity Search with CLIP\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "1. **Load** the CLIP embeddings computed in Notebook 03\n",
    "2. **Select or upload** a query image\n",
    "3. **Find** museum artworks that are visually similar to your image\n",
    "4. **Explore** how CLIP understands visual similarity\n",
    "\n",
    "---\n",
    "\n",
    "## Text Search vs. Image Search\n",
    "\n",
    "| | Text Search (Notebook 03) | Image Search (this notebook) |\n",
    "|--|--------------------------|------------------------------|\n",
    "| **Query** | `\"a river landscape\"` | üñºÔ∏è any image file |\n",
    "| **Encoded by** | CLIP text encoder | CLIP image encoder |\n",
    "| **Finds** | images matching the description | images with similar visual content |\n",
    "| **Use case** | explore by concept | \"find more like this\" |\n",
    "\n",
    "Both use the same embedding space ‚Äî the query vector just comes from a different encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import clip\n",
    "    CLIP_AVAILABLE = True\n",
    "    print(\"‚úì CLIP loaded!\")\n",
    "except ImportError:\n",
    "    CLIP_AVAILABLE = False\n",
    "    print(\"‚ùå CLIP not installed ‚Äî pip install git+https://github.com/openai/CLIP.git torch torchvision\")\n",
    "\n",
    "if CLIP_AVAILABLE:\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = 'cuda'\n",
    "        print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        DEVICE = 'mps'\n",
    "        print(\"‚úì Apple Silicon GPU (MPS)\")\n",
    "    else:\n",
    "        DEVICE = 'cpu'\n",
    "        print(\"‚ÑπÔ∏è  CPU mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/Distant_viewing\")\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION ‚Äî must match Notebooks 02 & 03!\n",
    "# ============================================================\n",
    "\n",
    "COLLECTION_NAME = \"Museum of Gothenburg\"\n",
    "THEME           = \"art\"\n",
    "SEARCH_KEYWORD  = None\n",
    "MODEL_NAME      = \"ViT-B/32\"\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "_parts = [COLLECTION_NAME.replace(' ', '_')]\n",
    "if THEME:          _parts.append(THEME)\n",
    "if SEARCH_KEYWORD: _parts.append(SEARCH_KEYWORD)\n",
    "_folder = \"_\".join(_parts)\n",
    "\n",
    "COLLECTION_IMAGES_DIR      = DATA_DIR / \"images\"     / _folder\n",
    "COLLECTION_EMBEDDINGS_FILE = DATA_DIR / \"embeddings\" / _folder / f\"{_folder}_clip_embeddings.npz\"\n",
    "CLIP_MODEL_DIR             = PROJECT_ROOT / \"models\" / \"CLIP\"\n",
    "\n",
    "print(f\"Images     : {COLLECTION_IMAGES_DIR}\")\n",
    "print(f\"Embeddings : {COLLECTION_EMBEDDINGS_FILE}\")\n",
    "print(f\"Exists     : {COLLECTION_EMBEDDINGS_FILE.exists()}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Load Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-clip",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLIP_AVAILABLE:\n",
    "    print(f\"Loading '{MODEL_NAME}' from {CLIP_MODEL_DIR}...\")\n",
    "    model, preprocess = clip.load(MODEL_NAME, device=DEVICE,\n",
    "                                  download_root=str(CLIP_MODEL_DIR))\n",
    "    model.eval()\n",
    "    print(f\"‚úì Model ready on {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan data/embeddings/ for collections that have computed embeddings\n",
    "_emb_root = DATA_DIR / \"embeddings\"\n",
    "_available = sorted([\n",
    "    d.name for d in _emb_root.iterdir()\n",
    "    if d.is_dir() and any(d.glob(\"*.npz\"))\n",
    "]) if _emb_root.exists() else []\n",
    "\n",
    "if not _available:\n",
    "    print(\"‚ö†Ô∏è  No embeddings found in data/embeddings/\")\n",
    "    print(\"   Run Notebook 03 first to compute embeddings.\")\n",
    "    collection_selector = None\n",
    "else:\n",
    "    _default = _folder if _folder in _available else _available[0]\n",
    "\n",
    "    collection_selector = widgets.Dropdown(\n",
    "        options=_available,\n",
    "        value=_default,\n",
    "        description=\"Collection:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"550px\"),\n",
    "    )\n",
    "\n",
    "    _info = widgets.HTML(value=\"\")\n",
    "\n",
    "    def _update_info(change=None):\n",
    "        sel     = collection_selector.value\n",
    "        npz     = next((_emb_root / sel).glob(\"*.npz\"), None)\n",
    "        img_dir = DATA_DIR / \"images\" / sel\n",
    "        n_emb   = \"\"\n",
    "        if npz:\n",
    "            try:\n",
    "                d = __import__(\"numpy\").load(npz, allow_pickle=True)\n",
    "                n_emb = f\"{len(d['filenames'])} embeddings\"\n",
    "            except Exception:\n",
    "                n_emb = \"embeddings file unreadable\"\n",
    "        n_img = len(list(img_dir.glob(\"*.jpg\"))) if img_dir.exists() else 0\n",
    "        _info.value = (\n",
    "            f\"<span style='color:grey'>{n_emb}\"\n",
    "            + (f\" ¬∑ {n_img} images locally\" if n_img else \" ¬∑ images not downloaded\")\n",
    "            + \"</span>\"\n",
    "        )\n",
    "\n",
    "    collection_selector.observe(lambda c: _update_info(c), names=\"value\")\n",
    "    _update_info()\n",
    "\n",
    "    display(widgets.VBox([collection_selector, _info]))\n",
    "    print(\"Select a collection, then run the next cell to load its embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve paths from the dropdown above (falls back to paths cell if widget not shown)\n",
    "_sel = collection_selector.value if collection_selector else _folder\n",
    "\n",
    "COLLECTION_IMAGES_DIR      = DATA_DIR / \"images\"     / _sel\n",
    "COLLECTION_EMBEDDINGS_FILE = next((DATA_DIR / \"embeddings\" / _sel).glob(\"*.npz\"), None)\n",
    "\n",
    "print(f\"Collection : {_sel}\")\n",
    "print(f\"Images     : {COLLECTION_IMAGES_DIR}\")\n",
    "print(f\"Embeddings : {COLLECTION_EMBEDDINGS_FILE}\")\n",
    "print()\n",
    "\n",
    "if COLLECTION_EMBEDDINGS_FILE and COLLECTION_EMBEDDINGS_FILE.exists():\n",
    "    data = np.load(COLLECTION_EMBEDDINGS_FILE, allow_pickle=True)\n",
    "    image_embeddings = torch.tensor(data['embeddings'], dtype=torch.float32).to(DEVICE)\n",
    "    image_filenames  = data['filenames']\n",
    "    print(f\"‚úì Loaded {len(image_filenames)} embeddings  {image_embeddings.shape}\")\n",
    "else:\n",
    "    image_embeddings = None\n",
    "    image_filenames  = None\n",
    "    print(\"‚ö†Ô∏è  No embeddings found ‚Äî run Notebook 03 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Choose a Query Image\n",
    "\n",
    "You can use **any image** as your query ‚Äî a painting, a photograph, even a smartphone photo.\n",
    "CLIP encodes it into the same 512-dim space as the collection images and finds the closest matches.\n",
    "\n",
    "### Option A ‚Äî Upload a file (works in Colab & JupyterLab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: upload from your computer\n",
    "upload = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "display(upload)\n",
    "print(\"Click 'Upload' and select an image file, then run the next cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-uploaded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Run this after uploading a file above\n",
    "if upload.value:\n",
    "    file_info = upload.value[0]          # ipywidgets 8+: tuple of dicts\n",
    "    name      = file_info['name']\n",
    "    img_bytes = file_info['content']\n",
    "    query_image = PILImage.open(io.BytesIO(img_bytes)).convert('RGB')\n",
    "    print(f\"‚úì Loaded: {name}  ({query_image.size[0]}√ó{query_image.size[1]})\")\n",
    "    display(query_image.resize((300, int(300 * query_image.size[1] / query_image.size[0]))))\n",
    "else:\n",
    "    query_image = None\n",
    "    print(\"No file uploaded yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Find Similar Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_similarity_search(query_img, top_k=10, exclude_self=True):\n",
    "    \"\"\"\n",
    "    Find images in the collection most similar to query_img.\n",
    "\n",
    "    Parameters:\n",
    "        query_img:    PIL Image to use as the query\n",
    "        top_k:        Number of results\n",
    "        exclude_self: If True, skip the exact query image (useful for Option B)\n",
    "\n",
    "    Returns:\n",
    "        List of (filename, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    if image_embeddings is None:\n",
    "        print(\"‚ùå No embeddings ‚Äî run the load cell above\")\n",
    "        return []\n",
    "\n",
    "    # Encode the query image with CLIP\n",
    "    with torch.no_grad():\n",
    "        tensor    = preprocess(query_img).unsqueeze(0).to(DEVICE)\n",
    "        query_emb = model.encode_image(tensor).float()\n",
    "        query_emb = query_emb / query_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Dot product against all collection embeddings\n",
    "    sims = (image_embeddings @ query_emb.T).squeeze()\n",
    "\n",
    "    # Sort and take top-k\n",
    "    top_idx = sims.argsort(descending=True)\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        fname = str(image_filenames[idx.item()])\n",
    "        score = sims[idx].item()\n",
    "        if exclude_self and score > 0.9999:\n",
    "            continue          # skip near-identical match (the image itself)\n",
    "        results.append((fname, score))\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if CLIP_AVAILABLE and image_embeddings is not None and query_image is not None:\n",
    "    print(\"üîç Searching for similar images...\")\n",
    "    similar = image_similarity_search(query_image, top_k=10)\n",
    "\n",
    "    print(f\"\\nTop {len(similar)} most similar images:\\n\")\n",
    "    for i, (fname, score) in enumerate(similar, 1):\n",
    "        print(f\"{i:2}. {score:.4f}  {Path(fname).name[:60]}\")\n",
    "else:\n",
    "    print(\"‚ùå Make sure CLIP is loaded, embeddings are loaded, and a query image is selected.\")\n",
    "    similar = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-similar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the most similar images\n",
    "def show_similar(results, max_show=5):\n",
    "    for fname, score in results[:max_show]:\n",
    "        img_path = COLLECTION_IMAGES_DIR / Path(fname).name\n",
    "        print(f\"\\nSimilarity: {score:.4f}  |  {Path(fname).name[:60]}\")\n",
    "        if img_path.exists():\n",
    "            img = PILImage.open(img_path)\n",
    "            ratio = min(350 / img.width, 350 / img.height)\n",
    "            if ratio < 1:\n",
    "                img = img.resize((int(img.width*ratio), int(img.height*ratio)),\n",
    "                                 PILImage.Resampling.LANCZOS)\n",
    "            display(img)\n",
    "        else:\n",
    "            print(\"  [file not found locally]\")\n",
    "\n",
    "if similar:\n",
    "    show_similar(similar, max_show=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Explore\n",
    "\n",
    "- Try **different query images** ‚Äî what makes two artworks \"similar\" to CLIP?\n",
    "- Compare CLIP similarity to your own intuition\n",
    "- Use a **modern photo** as the query ‚Äî can CLIP find historical artworks that match?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "1. **Used an image as a query** ‚Äî encoding it with CLIP's image encoder\n",
    "2. **Found similar artworks** by dot-product similarity in the shared embedding space\n",
    "3. **Explored** what \"visual similarity\" means to a neural network\n",
    "\n",
    "### How It Differs from Text Search\n",
    "\n",
    "Both text and image search land in the same embedding space.\n",
    "The only difference is *how the query is encoded* ‚Äî text encoder vs. image encoder.\n",
    "This is what makes CLIP so powerful: you can mix-and-match query types.\n",
    "\n",
    "### Further Ideas\n",
    "\n",
    "- **Cross-modal**: encode a text description, find similar images, then use the top image\n",
    "  as a new query to find even more similar images\n",
    "- **Clustering**: group embeddings by cosine similarity to discover visual themes\n",
    "  in the collection without any labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh-workshop",
   "language": "python",
   "name": "dh-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
