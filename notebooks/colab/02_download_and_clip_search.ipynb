{
 "cells": [
  {
   "cell_type": "code",
   "id": "colab-mount-drive",
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "# If already mounted this will show \"Drive is already mounted\" ‚Äî that's fine.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "colab-install-deps",
   "metadata": {},
   "source": [
    "# Install packages that are not pre-installed in Colab\n",
    "# (torch, torchvision, numpy, Pillow, requests are already available)\n",
    "!pip install -q git+https://github.com/openai/CLIP.git ftfy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# Download Uppsala Collection & Semantic Image Search with CLIP\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "1. **Download** the Uppsala University collection from Europeana (thumbnail resolution)\n",
    "2. **Learn** how CLIP connects images and text\n",
    "3. **Search** the collection using natural language (e.g., \"waterbody\", \"portrait\")\n",
    "4. **Explore** how different queries find different images\n",
    "\n",
    "---\n",
    "\n",
    "## What is CLIP?\n",
    "\n",
    "**CLIP** (Contrastive Language-Image Pre-training) is a neural network trained by OpenAI that learns to connect images and text. It can:\n",
    "\n",
    "- **Understand images** by converting them into numerical representations (embeddings)\n",
    "- **Understand text** by converting descriptions into the same embedding space\n",
    "- **Match** images and text by measuring how similar their embeddings are\n",
    "\n",
    "This allows us to search for images using natural language descriptions like:\n",
    "- \"a painting of a stormy sea\"\n",
    "- \"winter landscape with snow\"\n",
    "- \"flowers in a vase\"\n",
    "\n",
    "### How CLIP Works\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Input\n",
    "        IMG[\"üñºÔ∏è Image\"]\n",
    "        TXT[\"üìù Text\\n'a river landscape'\"]\n",
    "    end\n",
    "    \n",
    "    subgraph CLIP[\"CLIP Model\"]\n",
    "        IE[\"Image\\nEncoder\"]\n",
    "        TE[\"Text\\nEncoder\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Embeddings[\"Embedding Space\"]\n",
    "        IV[\"[0.12, -0.45, 0.78, ...]\"]\n",
    "        TV[\"[0.11, -0.42, 0.81, ...]\"]\n",
    "    end\n",
    "    \n",
    "    SIM[\"üìä Cosine\\nSimilarity\\n= 0.94\"]\n",
    "    \n",
    "    IMG --> IE --> IV\n",
    "    TXT --> TE --> TV\n",
    "    IV --> SIM\n",
    "    TV --> SIM\n",
    "```\n",
    "\n",
    "The key insight: **similar concepts end up close together** in embedding space, whether they come from images or text!\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-calculated Embeddings\n",
    "\n",
    "Computing image embeddings requires significant computational resources (ideally a GPU). For this workshop, we use **pre-calculated embeddings**:\n",
    "\n",
    "- Image embeddings were computed beforehand by the instructor\n",
    "- You only need to compute **text embeddings** (fast on any laptop)\n",
    "- This makes the workshop accessible on any hardware!\n",
    "\n",
    "If you want to compute your own embeddings, see **Notebook 04 (Advanced)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "# Standard library imports\nimport os\nimport json\nimport time\nfrom pathlib import Path\n\n# External libraries\nimport numpy as np\nimport requests\nfrom PIL import Image as PILImage\nfrom IPython.display import display, Image, HTML\n\n# Import CLIP\ntry:\n    import torch\n    import clip\n    CLIP_AVAILABLE = True\n    print(f\"‚úì CLIP loaded successfully!\")\nexcept ImportError:\n    CLIP_AVAILABLE = False\n    print(\"‚ö†Ô∏è CLIP not installed.\")\n    print(\"  Install with: pip install git+https://github.com/openai/CLIP.git torch torchvision\")\n\n# Select compute device: CUDA GPU > Apple Silicon GPU > CPU\nif CLIP_AVAILABLE:\n    if torch.cuda.is_available():\n        DEVICE = 'cuda'\n        print(f\"‚úì NVIDIA GPU (CUDA): {torch.cuda.get_device_name(0)}\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        DEVICE = 'mps'\n        print(\"‚úì Apple Silicon GPU (MPS) ‚Äî good performance!\")\n    else:\n        DEVICE = 'cpu'\n        print(\"‚ÑπÔ∏è No GPU detected. Using CPU ‚Äî fine for text search in this workshop.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/Distant_viewing\")\n",
    "\n",
    "# ============================================================\n",
    "# Collection to work with ‚Äî change this to switch collections!\n",
    "# ============================================================\n",
    "COLLECTION_NAME = \"Museum of Gothenburg\"  # <-- Change this!\n",
    "# ============================================================\n",
    "\n",
    "safe_name = COLLECTION_NAME.lower().replace(' ', '_')\n",
    "COLLECTION_IMAGES_DIR      = PROJECT_ROOT / \"data\" / \"images\"     / COLLECTION_NAME\n",
    "COLLECTION_EMBEDDINGS_FILE = PROJECT_ROOT / \"data\" / \"embeddings\" / COLLECTION_NAME / f\"{safe_name}_clip_embeddings.npz\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "# Create base directories\n",
    "(PROJECT_ROOT / \"data\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "(PROJECT_ROOT / \"data\" / \"embeddings\" / COLLECTION_NAME).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Collection:   {COLLECTION_NAME}\")\n",
    "print(f\"Images:       {COLLECTION_IMAGES_DIR}\")\n",
    "print(f\"Embeddings:   {COLLECTION_EMBEDDINGS_FILE}\")\n",
    "print(f\"Embeddings exist: {COLLECTION_EMBEDDINGS_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Download from a Swedish collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0bc7e0",
   "metadata": {},
   "source": [
    "  1. Swedish National Heritage Board                            (1,414,618 items)\n",
    "  2. Nordic Museum Foundation                                   (351,553 items)\n",
    "  3. Malm√∂ Museum                                               (288,308 items)\n",
    "  4. Museum of Ethnography                                      (271,525 items)\n",
    "  5. Museum of World Culture                                    (230,176 items)\n",
    "  6. Upplands Museum                                            (218,928 items)\n",
    "  7. Jamtli                                                     (173,095 items)\n",
    "  8. Museum of Gothenburg                                       (171,960 items)\n",
    "  9. Swedish National Museum of Science and Technology          (150,809 items)\n",
    " 10. Swedish Railway Museum                                     (145,093 items)\n",
    " 11. Naval Museum                                               (143,623 items)\n",
    " 12. Bohusl√§n Museum                                            (140,521 items)\n",
    " 13. G√§vleborg County Museum                                    (133,207 items)\n",
    " 14. Kulturen                                                   (130,732 items)\n",
    " 15. √ñrebro County Museum                                       (125,272 items)\n",
    " 16. V√§sterg√∂tlands Museum                                      (117,479 items)\n",
    " 17. Army Museum                                                (114,513 items)\n",
    " 18. S√∂rmland Museum                                            (107,177 items)\n",
    " 19. National Maritime Museum                                   (103,829 items)\n",
    " 20. V√§nersborgs museum                                         (103,249 items)\n",
    " 21. The Museum of Mediterranean and Near Eastern Antiquities... (74,761 items)\n",
    " 22. Museum of Far Eastern Antiquities                          (74,426 items)\n",
    " 23. Uppsala University                                         (74,233 items)\n",
    " 24. H√§lsinglands Museum                                        (73,678 items)\n",
    " 25. Swedish Centre for Architecture and Design                 (67,581 items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Setup (same as Notebook 01)\n",
    "BASE_URL = \"https://api.europeana.eu/record/v2\"\n",
    "\n",
    "# Load API key\n",
    "NOTEBOOK_DIR = Path(\".\").resolve()\n",
    "API_KEY_LOCATIONS = [\n",
    "    NOTEBOOK_DIR / \"api-key-europeana.txt\",\n",
    "    PROJECT_ROOT / \"misc\" / \"api-key-europeana.txt\",\n",
    "]\n",
    "\n",
    "API_KEY = \"api2demo\"\n",
    "for key_file in API_KEY_LOCATIONS:\n",
    "    if key_file.exists():\n",
    "        with open(key_file, 'r') as f:\n",
    "            custom_key = f.read().strip()\n",
    "            if custom_key and custom_key != \"api2demo\":\n",
    "                API_KEY = custom_key\n",
    "                print(f\"‚úì API key loaded from {key_file}\")\n",
    "                break\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Using demo API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (from Notebook 01)\n",
    "\n",
    "def search_europeana(query=\"*\", rows=12, reusability=\"open\", qf=None, profile=\"rich\", cursor=None):\n",
    "    \"\"\"Search the Europeana collection.\"\"\"\n",
    "    url = f\"{BASE_URL}/search.json\"\n",
    "    params = {\n",
    "        \"wskey\": API_KEY,\n",
    "        \"query\": query,\n",
    "        \"rows\": min(rows, 100),\n",
    "        \"profile\": profile\n",
    "    }\n",
    "    if reusability:\n",
    "        params[\"reusability\"] = reusability\n",
    "    if qf:\n",
    "        params[\"qf\"] = qf\n",
    "    if cursor:\n",
    "        params[\"cursor\"] = cursor\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_item_title(item):\n",
    "    if 'title' in item and item['title']:\n",
    "        return item['title'][0] if isinstance(item['title'], list) else item['title']\n",
    "    return \"Untitled\"\n",
    "\n",
    "\n",
    "def get_item_preview(item):\n",
    "    if 'edmPreview' in item and item['edmPreview']:\n",
    "        return item['edmPreview'][0] if isinstance(item['edmPreview'], list) else item['edmPreview']\n",
    "    return None\n",
    "\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    if not name:\n",
    "        return \"unknown\"\n",
    "    safe = \"\".join(c for c in name if c.isalnum() or c in ' ._-')\n",
    "    return safe.strip()[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-uppsala",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the selected collection has in Europeana\n",
    "print(f\"Checking {COLLECTION_NAME} collection...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = search_europeana(\n",
    "    query=\"*\",\n",
    "    rows=5,\n",
    "    qf=[f'DATA_PROVIDER:\"{COLLECTION_NAME}\"', \"TYPE:IMAGE\"],\n",
    "    reusability=\"open\"\n",
    ")\n",
    "\n",
    "if result and result.get('success'):\n",
    "    print(f\"‚úì {COLLECTION_NAME} has {result['totalResults']:,} open images\")\n",
    "    print(\"\\nSample items:\")\n",
    "    for item in result['items'][:20]:\n",
    "        print(f\"  - {get_item_title(item)[:60]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-uppsala",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE: Download images from the selected collection\n",
    "# ============================================================\n",
    "\n",
    "# How many images to download?\n",
    "MAX_DOWNLOAD = 100000  # <-- Start small, increase if needed\n",
    "\n",
    "# Actually download? (Set to True when ready)\n",
    "DO_DOWNLOAD = True  # <-- Change to True to download\n",
    "\n",
    "# Only download paintings? (Filters out photographs etc.)\n",
    "FILTER_PAINTINGS = True  # <-- Set to True to filter for paintings only\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "if DO_DOWNLOAD:\n",
    "    print(f\"üì• Downloading up to {MAX_DOWNLOAD} images from {COLLECTION_NAME}...\")\n",
    "    print(f\"   Saving to: {COLLECTION_IMAGES_DIR}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    COLLECTION_IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build filters\n",
    "    qf = [f'DATA_PROVIDER:\"{COLLECTION_NAME}\"', \"TYPE:IMAGE\"]\n",
    "    if FILTER_PAINTINGS:\n",
    "        qf.append(\"what:m√•lning\")\n",
    "        print(\"   Filter: paintings only\")\n",
    "\n",
    "    # Search for images\n",
    "    results = search_europeana(\n",
    "        query=\"*\",\n",
    "        rows=MAX_DOWNLOAD,\n",
    "        qf=qf,\n",
    "        reusability=\"open\"\n",
    "    )\n",
    "\n",
    "    if results and results.get('items'):\n",
    "        downloaded = 0\n",
    "\n",
    "        for i, item in enumerate(results['items'], 1):\n",
    "            title = get_item_title(item)\n",
    "            preview_url = get_item_preview(item)\n",
    "            item_id = item.get('id', 'unknown').replace('/', '_')\n",
    "\n",
    "            if not preview_url:\n",
    "                continue\n",
    "\n",
    "            # Create filename\n",
    "            safe_title = sanitize_filename(title)[:40]\n",
    "            filename = f\"{item_id}_{safe_title}.jpg\"\n",
    "            filepath = COLLECTION_IMAGES_DIR / filename\n",
    "\n",
    "            # Skip if exists\n",
    "            if filepath.exists():\n",
    "                downloaded += 1\n",
    "                continue\n",
    "\n",
    "            # Download\n",
    "            try:\n",
    "                response = requests.get(preview_url, timeout=20)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "\n",
    "                downloaded += 1\n",
    "                if downloaded % 20 == 0:\n",
    "                    print(f\"  Downloaded {downloaded}/{len(results['items'])} images...\")\n",
    "\n",
    "                time.sleep(0.3)  # Be nice to the server\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        print(f\"\\n‚úì Downloaded {downloaded} images to {COLLECTION_IMAGES_DIR}\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Download skipped. Set DO_DOWNLOAD = True to download images.\")\n",
    "\n",
    "    # Check if we already have images\n",
    "    if COLLECTION_IMAGES_DIR.exists():\n",
    "        existing = list(COLLECTION_IMAGES_DIR.glob('*.jpg'))\n",
    "        print(f\"   Found {len(existing)} existing images in {COLLECTION_IMAGES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clip-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Load CLIP Model and Compute Embeddings\n",
    "\n",
    "Now let's load CLIP, compute embeddings for the images you downloaded, and load them for search.\n",
    "\n",
    "- If embeddings **already exist** (e.g. pre-calculated by the instructor), the compute step is skipped automatically.\n",
    "- If they **don't exist yet**, they are computed from the images in `COLLECTION_IMAGES_DIR` and saved for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-clip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP model configuration\n",
    "MODEL_NAME = 'ViT-B/32'  # Good balance of speed and quality\n",
    "\n",
    "if CLIP_AVAILABLE:\n",
    "    print(f\"Loading CLIP model '{MODEL_NAME}'...\")\n",
    "    model, preprocess = clip.load(MODEL_NAME, device=DEVICE)\n",
    "    model.eval()\n",
    "    print(f\"‚úì Model loaded on {DEVICE}\")\n",
    "else:\n",
    "    print(\"‚ùå CLIP not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jq1d1deb5le",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CLIP embeddings for downloaded images\n",
    "# Skipped automatically if the embeddings file already exists.\n",
    "\n",
    "BATCH_SIZE = 32  # Reduce if you run out of memory\n",
    "\n",
    "if not CLIP_AVAILABLE:\n",
    "    print(\"‚ùå CLIP not available ‚Äî cannot compute embeddings\")\n",
    "elif COLLECTION_EMBEDDINGS_FILE.exists():\n",
    "    print(f\"‚úì Embeddings already exist: {COLLECTION_EMBEDDINGS_FILE}\")\n",
    "    print(\"  Delete the file and re-run this cell to recompute.\")\n",
    "else:\n",
    "    image_files = sorted(\n",
    "        f for f in COLLECTION_IMAGES_DIR.glob('*')\n",
    "        if f.suffix.lower() in ('.jpg', '.jpeg', '.png', '.webp')\n",
    "    )\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"‚ö†Ô∏è No images found in {COLLECTION_IMAGES_DIR}\")\n",
    "        print(\"   Run Part 2 first to download images.\")\n",
    "    else:\n",
    "        print(f\"Computing embeddings for {len(image_files)} images...\")\n",
    "        print(f\"  Model: {MODEL_NAME}  |  Device: {DEVICE}  |  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "        all_embeddings = []\n",
    "        all_filenames  = []\n",
    "        failed         = []\n",
    "\n",
    "        for i in range(0, len(image_files), BATCH_SIZE):\n",
    "            batch_files   = image_files[i:i + BATCH_SIZE]\n",
    "            batch_tensors = []\n",
    "            batch_names   = []\n",
    "\n",
    "            for img_path in batch_files:\n",
    "                try:\n",
    "                    img = PILImage.open(img_path).convert('RGB')\n",
    "                    batch_tensors.append(preprocess(img))\n",
    "                    batch_names.append(img_path.name)\n",
    "                except Exception as e:\n",
    "                    failed.append((img_path.name, str(e)))\n",
    "\n",
    "            if batch_tensors:\n",
    "                with torch.no_grad():\n",
    "                    batch = torch.stack(batch_tensors).to(DEVICE)\n",
    "                    emb   = model.encode_image(batch).float()\n",
    "                    emb   = emb / emb.norm(dim=-1, keepdim=True)\n",
    "                all_embeddings.append(emb.cpu().numpy())\n",
    "                all_filenames.extend(batch_names)\n",
    "\n",
    "            done = min(i + BATCH_SIZE, len(image_files))\n",
    "            if done % (BATCH_SIZE * 10) == 0 or done == len(image_files):\n",
    "                print(f\"  Processed {done}/{len(image_files)} images...\")\n",
    "\n",
    "        embeddings_array = np.vstack(all_embeddings)\n",
    "\n",
    "        np.savez_compressed(\n",
    "            COLLECTION_EMBEDDINGS_FILE,\n",
    "            embeddings=embeddings_array,\n",
    "            filenames=np.array(all_filenames, dtype=object),\n",
    "            model_name=MODEL_NAME,\n",
    "            embedding_dim=embeddings_array.shape[1],\n",
    "        )\n",
    "\n",
    "        if failed:\n",
    "            print(f\"  ‚ö†Ô∏è Failed to process {len(failed)} images\")\n",
    "\n",
    "        print(f\"\\n‚úì Saved {len(all_filenames)} embeddings ‚Üí {COLLECTION_EMBEDDINGS_FILE}\")\n",
    "        print(f\"  Embeddings shape: {embeddings_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-embeddings",
   "metadata": {},
   "outputs": [],
   "source": "# Load pre-calculated embeddings\n# To override the path, uncomment and edit:\n# COLLECTION_EMBEDDINGS_FILE = PROJECT_ROOT / \"data\" / \"embeddings\" / \"my_embeddings.npz\"\n\nif COLLECTION_EMBEDDINGS_FILE.exists():\n    print(f\"Loading pre-calculated embeddings from {COLLECTION_EMBEDDINGS_FILE}...\")\n\n    data = np.load(COLLECTION_EMBEDDINGS_FILE, allow_pickle=True)\n    image_embeddings = data['embeddings']\n    image_filenames = data['filenames']\n    saved_model = str(data.get('model_name', 'unknown'))\n\n    print(f\"‚úì Loaded {len(image_filenames)} image embeddings\")\n    print(f\"  Embeddings shape: {image_embeddings.shape}\")\n    print(f\"  Model used: {saved_model}\")\n\n    # Convert to torch tensor\n    if CLIP_AVAILABLE:\n        image_embeddings = torch.tensor(image_embeddings, dtype=torch.float32).to(DEVICE)\nelse:\n    image_embeddings = None\n    image_filenames = None\n    print(f\"‚ö†Ô∏è Pre-calculated embeddings not found at {COLLECTION_EMBEDDINGS_FILE}\")\n    print(\"   Ask your instructor for the embeddings file, or\")\n    print(\"   see Notebook 04 to compute your own (works on CPU too, but takes longer).\")"
  },
  {
   "cell_type": "markdown",
   "id": "search-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Semantic Search with CLIP\n",
    "\n",
    "Now the fun part! Let's search for images using natural language.\n",
    "\n",
    "### Understanding Cosine Similarity\n",
    "\n",
    "Cosine similarity measures how similar two vectors are:\n",
    "\n",
    "- **1.0** = Identical direction (very similar)\n",
    "- **0.0** = Perpendicular (unrelated)\n",
    "- **-1.0** = Opposite direction (very different)\n",
    "\n",
    "CLIP embeddings are normalized, so cosine similarity = simple dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text_query):\n",
    "    \"\"\"\n",
    "    Convert a text query into a CLIP embedding.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize([text_query]).to(DEVICE)\n",
    "        text_embedding = model.encode_text(text_tokens).float()\n",
    "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
    "    return text_embedding\n",
    "\n",
    "\n",
    "def encode_text_ensemble(text_queries):\n",
    "    \"\"\"\n",
    "    Encode multiple text queries and average their embeddings.\n",
    "    This technique (prompt ensembling) often gives better results.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize(text_queries).to(DEVICE)\n",
    "        text_embeddings = model.encode_text(text_tokens).float()\n",
    "        mean_embedding = text_embeddings.mean(dim=0, keepdim=True)\n",
    "        mean_embedding = mean_embedding / mean_embedding.norm(dim=-1, keepdim=True)\n",
    "    return mean_embedding\n",
    "\n",
    "\n",
    "def semantic_search(query, top_k=10, use_ensemble=True):\n",
    "    \"\"\"\n",
    "    Search images using natural language.\n",
    "    \n",
    "    Parameters:\n",
    "        query: Natural language search query\n",
    "        top_k: Number of results to return\n",
    "        use_ensemble: If True, create query variations for better results\n",
    "    \n",
    "    Returns:\n",
    "        List of (filename, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    if image_embeddings is None:\n",
    "        print(\"‚ùå No image embeddings loaded\")\n",
    "        return []\n",
    "    \n",
    "    if use_ensemble:\n",
    "        queries = [\n",
    "            query,\n",
    "            f\"a painting of {query}\",\n",
    "            f\"an image showing {query}\",\n",
    "            f\"a photograph of {query}\",\n",
    "            f\"{query}, artwork\"\n",
    "        ]\n",
    "        text_embedding = encode_text_ensemble(queries)\n",
    "    else:\n",
    "        text_embedding = encode_text(query)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = (image_embeddings @ text_embedding.T).squeeze()\n",
    "    \n",
    "    # Get top-k\n",
    "    top_indices = similarities.argsort(descending=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        filename = str(image_filenames[idx.item()])\n",
    "        score = similarities[idx].item()\n",
    "        results.append((filename, score))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_path(filename):\n",
    "    \"\"\"Find the full path to an image file.\"\"\"\n",
    "    # Try collection images directory\n",
    "    full_path = COLLECTION_IMAGES_DIR / filename\n",
    "    if full_path.exists():\n",
    "        return full_path\n",
    "\n",
    "    # Try just the filename\n",
    "    full_path = COLLECTION_IMAGES_DIR / Path(filename).name\n",
    "    if full_path.exists():\n",
    "        return full_path\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def display_results(results, max_display=5):\n",
    "    \"\"\"\n",
    "    Display search results as images.\n",
    "    \"\"\"\n",
    "    displayed = 0\n",
    "\n",
    "    for filename, score in results[:max_display]:\n",
    "        img_path = find_image_path(filename)\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Similarity: {score:.4f}\")\n",
    "        print(f\"File: {Path(filename).name}\")\n",
    "\n",
    "        if img_path and img_path.exists():\n",
    "            try:\n",
    "                img = PILImage.open(img_path)\n",
    "                # Resize for display\n",
    "                max_size = 400\n",
    "                ratio = min(max_size / img.width, max_size / img.height)\n",
    "                if ratio < 1:\n",
    "                    new_size = (int(img.width * ratio), int(img.height * ratio))\n",
    "                    img = img.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "                display(img)\n",
    "                displayed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Could not display: {e}\")\n",
    "        else:\n",
    "            print(f\"[Image file not found locally]\")\n",
    "\n",
    "    if displayed == 0:\n",
    "        print(\"\\n‚ö†Ô∏è No images could be displayed.\")\n",
    "        print(\"Make sure images are downloaded (Part 2) or ask your instructor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Try Semantic Search!\n",
    "\n",
    "Now let's search for images using natural language.\n",
    "\n",
    "### Tips for Better Queries\n",
    "\n",
    "| Instead of... | Try... | Why |\n",
    "|---------------|--------|-----|\n",
    "| `water` | `river flowing through landscape` | More specific |\n",
    "| `person` | `portrait of a woman` | Provides context |\n",
    "| `nature` | `forest with tall trees` | More descriptive |\n",
    "\n",
    "### Query Ideas\n",
    "\n",
    "**Water bodies:**\n",
    "- `\"river with trees\"`\n",
    "- `\"lake surrounded by mountains\"`\n",
    "- `\"ocean waves crashing\"`\n",
    "- `\"waterfall in nature\"`\n",
    "\n",
    "**People:**\n",
    "- `\"portrait of elderly man\"`\n",
    "- `\"group of people\"`\n",
    "- `\"person reading\"`\n",
    "\n",
    "**Nature:**\n",
    "- `\"winter landscape with snow\"`\n",
    "- `\"flowers in bloom\"`\n",
    "- `\"sunset over hills\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE: Search for images!\n",
    "# ============================================================\n",
    "\n",
    "SEARCH_QUERY = \"waterbody\"  # <-- CHANGE THIS!\n",
    "\n",
    "TOP_K = 10  # Number of results\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "if CLIP_AVAILABLE and image_embeddings is not None:\n",
    "    print(f\"üîç Searching for: '{SEARCH_QUERY}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = semantic_search(SEARCH_QUERY, top_k=TOP_K, use_ensemble=True)\n",
    "    \n",
    "    print(f\"\\nTop {len(results)} results:\")\n",
    "    for i, (filename, score) in enumerate(results, 1):\n",
    "        print(f\"{i:2}. {score:.4f} - {Path(filename).name[:50]}\")\n",
    "else:\n",
    "    print(\"‚ùå CLIP or embeddings not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top results\n",
    "if CLIP_AVAILABLE and image_embeddings is not None and results:\n",
    "    print(f\"\\nüñºÔ∏è Displaying top results for '{SEARCH_QUERY}':\\n\")\n",
    "    display_results(results, max_display=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Compare Different Queries\n",
    "\n",
    "See how different phrasings affect search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different ways to search for water\n",
    "QUERIES_TO_COMPARE = [\n",
    "    \"water\",\n",
    "    \"river\",\n",
    "    \"lake\",\n",
    "    \"ocean waves\",\n",
    "    \"waterfall\"\n",
    "]\n",
    "\n",
    "if CLIP_AVAILABLE and image_embeddings is not None:\n",
    "    print(\"Comparing queries - showing top result for each:\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for query in QUERIES_TO_COMPARE:\n",
    "        results = semantic_search(query, top_k=1, use_ensemble=False)\n",
    "        if results:\n",
    "            filename, score = results[0]\n",
    "            print(f\"'{query}'\")\n",
    "            print(f\"  ‚Üí {score:.4f} - {Path(filename).name[:45]}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ADVANCED: Create custom prompt ensemble\n",
    "# ============================================================\n",
    "\n",
    "CUSTOM_PROMPTS = [\n",
    "    \"water near the shore\",\n",
    "    \"beach scene with ocean\",\n",
    "    \"coastal landscape\",\n",
    "    \"waves on the beach\",\n",
    "    \"seaside view\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "if CLIP_AVAILABLE and image_embeddings is not None:\n",
    "    print(\"Using custom prompt ensemble:\")\n",
    "    for p in CUSTOM_PROMPTS:\n",
    "        print(f\"  - {p}\")\n",
    "    print()\n",
    "    \n",
    "    # Encode and average\n",
    "    custom_embedding = encode_text_ensemble(CUSTOM_PROMPTS)\n",
    "    \n",
    "    # Search\n",
    "    similarities = (image_embeddings @ custom_embedding.T).squeeze()\n",
    "    top_indices = similarities.argsort(descending=True)[:10]\n",
    "    \n",
    "    print(\"Top 10 results:\")\n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        filename = str(image_filenames[idx.item()])\n",
    "        score = similarities[idx].item()\n",
    "        print(f\"{i:2}. {score:.4f} - {Path(filename).name[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Save Search Results\n",
    "\n",
    "Save your search results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_search_results(results, query, output_dir=None):\n",
    "    \"\"\"Save search results to a JSON file.\"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = DATA_DIR\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    safe_query = \"\".join(c for c in query if c.isalnum() or c in ' _-')[:40]\n",
    "    filename = f\"clip_search_{safe_query.replace(' ', '_')}.json\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    data = {\n",
    "        'query': query,\n",
    "        'model': MODEL_NAME,\n",
    "        'results': [\n",
    "            {'filename': f, 'similarity': s}\n",
    "            for f, s in results\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Saved {len(results)} results to {output_path}\")\n",
    "\n",
    "\n",
    "# Save your search results\n",
    "if 'results' in dir() and results:\n",
    "    save_search_results(results, SEARCH_QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **How to download** images from a specific Europeana collection (Uppsala University)\n",
    "2. **What CLIP is** and how it connects images and text\n",
    "3. **How to search** images using natural language queries\n",
    "4. **How prompt engineering** affects search results\n",
    "5. **How to compare** different queries\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Embeddings**: Numerical representations that capture meaning\n",
    "- **Cosine similarity**: Measures how similar two embeddings are\n",
    "- **Prompt ensembling**: Using multiple prompts for better results\n",
    "- **Semantic search**: Finding images by meaning, not keywords\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 03**: Upload your own photo and find similar images in the collection\n",
    "- **Notebook 04** (Advanced): Compute embeddings from your own images (requires GPU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh-workshop",
   "language": "python",
   "name": "dh-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
