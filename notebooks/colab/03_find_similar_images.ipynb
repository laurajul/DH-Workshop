{
 "cells": [
  {
   "cell_type": "code",
   "id": "colab-mount-drive",
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "# If already mounted this will show \"Drive is already mounted\" ‚Äî that's fine.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "colab-install-deps",
   "metadata": {},
   "source": [
    "# Install packages that are not pre-installed in Colab\n",
    "# (torch, torchvision, numpy, Pillow, requests are already available)\n",
    "!pip install -q git+https://github.com/openai/CLIP.git ftfy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": "# Find Similar Images: Upload Your Own Photo\n\nIn this notebook, you will:\n\n1. **Upload** a photo from your laptop\n2. **Compute** a CLIP embedding for your photo\n3. **Find** the most visually similar images in the collection\n4. **Explore** what \"visual similarity\" means to a neural network\n\n---\n\n## How It Works\n\n```mermaid\nflowchart LR\n    subgraph Your Photo\n        UP[\"üì∑ Upload\\nyour photo\"]\n    end\n    \n    subgraph CLIP\n        ENC[\"üß† CLIP\\nImage Encoder\"]\n    end\n    \n    subgraph Embeddings\n        YE[\"Your embedding\\n[0.12, -0.45, ...]\"]\n        CE[\"Collection embeddings\\n(pre-calculated)\"]\n    end\n    \n    subgraph Results\n        SIM[\"üìä Compare\\n(cosine similarity)\"]\n        RES[\"üñºÔ∏è Most similar\\nimages\"]\n    end\n    \n    UP --> ENC --> YE\n    YE --> SIM\n    CE --> SIM\n    SIM --> RES\n```\n\nThe key insight: CLIP learns to represent images in a way that captures **semantic content**, not just pixels. Two images of sunsets will have similar embeddings even if they have different colors or compositions."
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "# Standard library imports\nimport os\nimport json\nimport io\nfrom pathlib import Path\n\n# External libraries\nimport numpy as np\nfrom PIL import Image as PILImage\nfrom IPython.display import display, HTML, clear_output\n\n# Import CLIP\ntry:\n    import torch\n    import clip\n    CLIP_AVAILABLE = True\n    print(f\"‚úì CLIP loaded successfully!\")\nexcept ImportError:\n    CLIP_AVAILABLE = False\n    print(\"‚ö†Ô∏è CLIP not installed.\")\n    print(\"  Install with: pip install git+https://github.com/openai/CLIP.git torch torchvision\")\n\n# Import ipywidgets for file upload\ntry:\n    import ipywidgets as widgets\n    WIDGETS_AVAILABLE = True\n    print(\"‚úì ipywidgets available\")\nexcept ImportError:\n    WIDGETS_AVAILABLE = False\n    print(\"‚ö†Ô∏è ipywidgets not installed. Install with: pip install ipywidgets\")\n\n# Select compute device: CUDA GPU > Apple Silicon GPU > CPU\nif CLIP_AVAILABLE:\n    if torch.cuda.is_available():\n        DEVICE = 'cuda'\n        print(f\"‚úì NVIDIA GPU (CUDA): {torch.cuda.get_device_name(0)}\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        DEVICE = 'mps'\n        print(\"‚úì Apple Silicon GPU (MPS) ‚Äî good performance!\")\n    else:\n        DEVICE = 'cpu'\n        print(\"‚ÑπÔ∏è No GPU detected. Using CPU ‚Äî fine for single image encoding.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/Distant_viewing\")\n",
    "\n",
    "# ============================================================\n",
    "# Collection to work with ‚Äî change this to switch collections!\n",
    "# ============================================================\n",
    "COLLECTION_NAME = \"Museum of Gothenburg\"  # <-- Change this!\n",
    "# ============================================================\n",
    "\n",
    "safe_name = COLLECTION_NAME.lower().replace(' ', '_')\n",
    "COLLECTION_IMAGES_DIR      = PROJECT_ROOT / \"data\" / \"images\"     / COLLECTION_NAME\n",
    "COLLECTION_EMBEDDINGS_FILE = PROJECT_ROOT / \"data\" / \"embeddings\" / COLLECTION_NAME / f\"{safe_name}_clip_embeddings.npz\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Collection:   {COLLECTION_NAME}\")\n",
    "print(f\"Images:       {COLLECTION_IMAGES_DIR}\")\n",
    "print(f\"Embeddings:   {COLLECTION_EMBEDDINGS_FILE}\")\n",
    "print(f\"Embeddings exist: {COLLECTION_EMBEDDINGS_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-clip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "MODEL_NAME = 'ViT-B/32'\n",
    "\n",
    "if CLIP_AVAILABLE:\n",
    "    print(f\"Loading CLIP model '{MODEL_NAME}'...\")\n",
    "    model, preprocess = clip.load(MODEL_NAME, device=DEVICE)\n",
    "    model.eval()\n",
    "    print(f\"‚úì Model loaded on {DEVICE}\")\n",
    "else:\n",
    "    print(\"‚ùå CLIP not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-calculated collection embeddings\n",
    "collection_embeddings = None\n",
    "collection_filenames = None\n",
    "\n",
    "if COLLECTION_EMBEDDINGS_FILE.exists():\n",
    "    print(f\"Loading collection embeddings from {COLLECTION_EMBEDDINGS_FILE}...\")\n",
    "\n",
    "    data = np.load(COLLECTION_EMBEDDINGS_FILE, allow_pickle=True)\n",
    "    collection_embeddings = data['embeddings']\n",
    "    collection_filenames = data['filenames']\n",
    "\n",
    "    print(f\"‚úì Loaded {len(collection_filenames)} image embeddings\")\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    if CLIP_AVAILABLE:\n",
    "        collection_embeddings = torch.tensor(collection_embeddings, dtype=torch.float32).to(DEVICE)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Embeddings not found at {COLLECTION_EMBEDDINGS_FILE}\")\n",
    "    print(\"   Ask your instructor for the embeddings file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Upload Your Photo\n",
    "\n",
    "Use the widget below to select a photo from your laptop.\n",
    "\n",
    "**Supported formats:** JPG, PNG, WEBP, GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-widget",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file upload widget\n",
    "uploaded_image = None\n",
    "uploaded_embedding = None\n",
    "\n",
    "if WIDGETS_AVAILABLE:\n",
    "    # File upload widget\n",
    "    upload_widget = widgets.FileUpload(\n",
    "        accept='image/*',\n",
    "        multiple=False,\n",
    "        description='Choose Photo'\n",
    "    )\n",
    "    \n",
    "    # Output area for preview\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Process button\n",
    "    process_btn = widgets.Button(\n",
    "        description='Find Similar Images',\n",
    "        button_style='primary',\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Status label\n",
    "    status_label = widgets.Label(value='Upload a photo to get started')\n",
    "    \n",
    "    def on_upload_change(change):\n",
    "        global uploaded_image\n",
    "        \n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            if upload_widget.value:\n",
    "                # ipywidgets >= 8.0: value is a tuple; < 8.0: value is a dict\n",
    "                val = upload_widget.value\n",
    "                file_info = val[0] if isinstance(val, tuple) else list(val.values())[0]\n",
    "                content = file_info['content']\n",
    "                \n",
    "                try:\n",
    "                    # Load image\n",
    "                    uploaded_image = PILImage.open(io.BytesIO(bytes(content))).convert('RGB')\n",
    "                    \n",
    "                    # Display preview\n",
    "                    print(\"üì∑ Your uploaded photo:\")\n",
    "                    print(f\"   Size: {uploaded_image.size[0]} x {uploaded_image.size[1]} pixels\")\n",
    "                    print()\n",
    "                    \n",
    "                    # Resize for display\n",
    "                    display_img = uploaded_image.copy()\n",
    "                    max_size = 400\n",
    "                    ratio = min(max_size / display_img.width, max_size / display_img.height)\n",
    "                    if ratio < 1:\n",
    "                        new_size = (int(display_img.width * ratio), int(display_img.height * ratio))\n",
    "                        display_img = display_img.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "                    display(display_img)\n",
    "                    \n",
    "                    # Enable process button\n",
    "                    process_btn.disabled = False\n",
    "                    status_label.value = '‚úì Photo loaded! Click \"Find Similar Images\"'\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error loading image: {e}\")\n",
    "                    status_label.value = f'Error: {e}'\n",
    "    \n",
    "    upload_widget.observe(on_upload_change, names='value')\n",
    "    \n",
    "    # Display the widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML('<h3>Step 1: Choose a photo from your laptop</h3>'),\n",
    "        upload_widget,\n",
    "        status_label,\n",
    "        output\n",
    "    ]))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è File upload widget not available.\")\n",
    "    print(\"   You can manually specify an image path below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-md",
   "metadata": {},
   "source": [
    "### Alternative: Specify Image Path Manually\n",
    "\n",
    "If the upload widget doesn't work, you can specify the path to an image file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALTERNATIVE: Specify image path manually\n",
    "# ============================================================\n",
    "\n",
    "# Set this to the path of your image file\n",
    "MANUAL_IMAGE_PATH = None  # e.g., \"/home/user/photos/my_photo.jpg\"\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "if MANUAL_IMAGE_PATH:\n",
    "    path = Path(MANUAL_IMAGE_PATH)\n",
    "    if path.exists():\n",
    "        uploaded_image = PILImage.open(path).convert('RGB')\n",
    "        print(f\"‚úì Loaded image: {path.name}\")\n",
    "        print(f\"  Size: {uploaded_image.size[0]} x {uploaded_image.size[1]} pixels\")\n",
    "        \n",
    "        # Display preview\n",
    "        display_img = uploaded_image.copy()\n",
    "        max_size = 400\n",
    "        ratio = min(max_size / display_img.width, max_size / display_img.height)\n",
    "        if ratio < 1:\n",
    "            new_size = (int(display_img.width * ratio), int(display_img.height * ratio))\n",
    "            display_img = display_img.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "        display(display_img)\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compute-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Compute Embedding and Find Similar Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-embedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_embedding(image):\n",
    "    \"\"\"\n",
    "    Compute CLIP embedding for an image.\n",
    "    \n",
    "    Parameters:\n",
    "        image: PIL Image\n",
    "    \n",
    "    Returns:\n",
    "        Normalized embedding tensor\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Preprocess image\n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # Compute embedding\n",
    "        embedding = model.encode_image(image_tensor).float()\n",
    "        \n",
    "        # Normalize\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def find_similar_images(query_embedding, top_k=10):\n",
    "    \"\"\"\n",
    "    Find most similar images in the collection.\n",
    "    \n",
    "    Parameters:\n",
    "        query_embedding: The embedding of the query image\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (filename, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity\n",
    "    similarities = (collection_embeddings @ query_embedding.T).squeeze()\n",
    "    \n",
    "    # Get top-k\n",
    "    top_indices = similarities.argsort(descending=True)[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        filename = str(collection_filenames[idx.item()])\n",
    "        score = similarities[idx].item()\n",
    "        results.append((filename, score))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-similar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar images!\n",
    "similar_results = None\n",
    "\n",
    "if uploaded_image is not None and CLIP_AVAILABLE and collection_embeddings is not None:\n",
    "    print(\"üîç Computing embedding for your photo...\")\n",
    "    uploaded_embedding = compute_image_embedding(uploaded_image)\n",
    "    print(\"‚úì Embedding computed!\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üîç Finding similar images in the collection...\")\n",
    "    similar_results = find_similar_images(uploaded_embedding, top_k=10)\n",
    "    \n",
    "    print(f\"\\nüìä Top 10 most similar images:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, (filename, score) in enumerate(similar_results, 1):\n",
    "        print(f\"{i:2}. Similarity: {score:.4f} - {Path(filename).name[:45]}\")\n",
    "else:\n",
    "    if uploaded_image is None:\n",
    "        print(\"‚ö†Ô∏è Please upload a photo first (Part 2)\")\n",
    "    elif not CLIP_AVAILABLE:\n",
    "        print(\"‚ùå CLIP not available\")\n",
    "    else:\n",
    "        print(\"‚ùå Collection embeddings not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "display-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: View Results\n",
    "\n",
    "Let's see your uploaded photo alongside the most similar images from the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_collection_image(filename):\n",
    "    \"\"\"Find the full path to a collection image.\"\"\"\n",
    "    full_path = COLLECTION_IMAGES_DIR / filename\n",
    "    if full_path.exists():\n",
    "        return full_path\n",
    "\n",
    "    full_path = COLLECTION_IMAGES_DIR / Path(filename).name\n",
    "    if full_path.exists():\n",
    "        return full_path\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Display results side by side\n",
    "if similar_results and uploaded_image:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"YOUR PHOTO vs SIMILAR IMAGES FROM COLLECTION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Display your photo\n",
    "    print(\"\\nüì∑ Your uploaded photo:\")\n",
    "    display_img = uploaded_image.copy()\n",
    "    max_size = 300\n",
    "    ratio = min(max_size / display_img.width, max_size / display_img.height)\n",
    "    if ratio < 1:\n",
    "        new_size = (int(display_img.width * ratio), int(display_img.height * ratio))\n",
    "        display_img = display_img.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "    display(display_img)\n",
    "\n",
    "    # Display similar images\n",
    "    print(f\"\\nüñºÔ∏è Most similar images from the {COLLECTION_NAME} collection:\")\n",
    "\n",
    "    for i, (filename, score) in enumerate(similar_results[:5], 1):\n",
    "        print(f\"\\n--- #{i} Similarity: {score:.4f} ---\")\n",
    "        print(f\"File: {Path(filename).name}\")\n",
    "\n",
    "        img_path = find_collection_image(filename)\n",
    "        if img_path and img_path.exists():\n",
    "            try:\n",
    "                img = PILImage.open(img_path)\n",
    "                ratio = min(max_size / img.width, max_size / img.height)\n",
    "                if ratio < 1:\n",
    "                    new_size = (int(img.width * ratio), int(img.height * ratio))\n",
    "                    img = img.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "                display(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not display: {e}\")\n",
    "        else:\n",
    "            print(\"[Image file not found locally]\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Run the previous cells first to find similar images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Reflection\n",
    "\n",
    "### Questions to Consider\n",
    "\n",
    "1. **Are the results what you expected?** Why or why not?\n",
    "\n",
    "2. **What features do your photo and the similar images share?**\n",
    "   - Colors?\n",
    "   - Composition?\n",
    "   - Subject matter?\n",
    "   - Mood/atmosphere?\n",
    "\n",
    "3. **What does CLIP seem to focus on?**\n",
    "   - Does it prioritize semantic content (what's in the image)?\n",
    "   - Or visual style (how it looks)?\n",
    "\n",
    "4. **What are the limitations?**\n",
    "   - What kinds of photos might not work well?\n",
    "   - How might cultural bias affect the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "try-another",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE: Try with a different photo!\n",
    "# ============================================================\n",
    "\n",
    "# Go back to Part 2 and upload a different photo.\n",
    "# Then run the cells again to see how the results change.\n",
    "\n",
    "# Ideas to try:\n",
    "# - A landscape photo\n",
    "# - A portrait\n",
    "# - An abstract pattern\n",
    "# - A photo of an artwork you like\n",
    "\n",
    "print(\"üí° Try uploading different types of photos to explore\")\n",
    "print(\"   how CLIP understands visual similarity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Save Your Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your similarity results\n",
    "if similar_results:\n",
    "    output_path = DATA_DIR / \"my_similar_images.json\"\n",
    "    \n",
    "    data = {\n",
    "        'query_type': 'uploaded_image',\n",
    "        'model': MODEL_NAME,\n",
    "        'results': [\n",
    "            {'filename': f, 'similarity': s}\n",
    "            for f, s in similar_results\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Saved results to {output_path}\")\n",
    "else:\n",
    "    print(\"No results to save yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": "---\n\n## Summary\n\nIn this notebook, you:\n\n1. **Uploaded** your own photo\n2. **Computed** a CLIP embedding for it\n3. **Found** the most visually similar images in the collection\n4. **Explored** what \"visual similarity\" means to a neural network\n\n### Key Takeaways\n\n- CLIP captures **semantic** similarity, not just pixel similarity\n- The same model can match images to text AND images to images\n- Results may reveal **unexpected connections** between artworks\n- Neural networks have **biases** based on their training data\n\n### Next Steps\n\n- **Notebook 04** (Advanced): Learn how to compute embeddings for your own image collections"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh-workshop",
   "language": "python",
   "name": "dh-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
